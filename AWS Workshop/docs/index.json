[{"uri":"/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Accelerate AI development with Amazon Bedrock API keys by Sofian Hamiti, Ajit Mahareddy, Massimiliano Angelino, Huong Nguyen, and Nakul Vankadari Ramesh on 08 JUL 2025 in Amazon Bedrock, Amazon Machine Learning, Announcements, Artificial Intelligence, Foundation models Permalink Comments Share\nToday, we’re excited to announce a significant improvement to the developer experience of Amazon Bedrock: API keys. API keys provide a new way to access the Amazon Bedrock APIs, streamlining the authentication process so that developers can focus on building rather than configuration.\nCamelAI is an open-source, modular framework for building intelligent multi-agent systems for data generation, world simulation, and task automation.\n“As a startup with limited resources, streamlined customer onboarding is critical to our success. The Amazon Bedrock API keys enable us to onboard enterprise customers in minutes rather than hours. With Bedrock, our customers can quickly provision access to leading AI models and seamlessly integrate them into CamelAI,”\nsaid Miguel Salinas, CTO, CamelAI. In this post, explore how API keys work and how you can start using them today.\nAPI key authentication Amazon Bedrock now provides API key access to streamline integration with tools and frameworks that expect API key-based authentication. The Amazon Bedrock and Amazon Bedrock runtime SDKs support API key authentication for methods including on-demand inference, provisioned throughput inference, model fine-tuning, distillation, and evaluation.\nThe diagram compares the default authentication process to Amazon Bedrock (in orange) with the API keys approach (in blue). In the default process, you must create an identity in AWS IAM Identity Center or AWS IAM, attach IAM policies to provide permissions to perform API operations, and generate credentials, which you can then use to make API calls. The grey boxes in the diagram highlight the steps that Amazon Bedrock now streamlines when generating an API key. Developers can now authenticate and access Amazon Bedrock APIs with minimal setup overhead.\nYou can generate API keys in the Amazon Bedrock console, choosing between two types.\nShort-term API keys use the IAM permissions from your current IAM principal and expire when your account’s session ends or can last up to 12 hours, whichever ends first. Short-term API keys use AWS Signature Version 4 for authentication. For continuous application use, you can implement API key refreshing following those examples and using your credential provider of choice.\nWhen you create a long-term API key, Amazon Bedrock automatically creates an IAM user and associates the key with it. You can set expiration times ranging from 1 day to no expiration. Amazon Bedrock attaches the AmazonBedrockLimitedAccess managed policy to the IAM user, and you can modify permissions as needed through the IAM service. These keys are specific to Amazon Bedrock and cannot be used with other AWS services. We recommend using temporary AWS IAM credentials or short-term API keys for setups that require a higher level of security, and long-term keys with expiration dates for exploring Amazon Bedrock.\nMaking Your First API Call Once you have access to foundation models, getting started with Amazon Bedrock API key is straightforward. Here’s how to make your first API call using the AWS SDK for Python (Boto3 SDK) and API keys: Generate an API key To generate an API key, follow these steps:\n1.Sign in to the AWS Management Console and open the Amazon Bedrock console 2.In the left navigation panel, select API keys 3.Choose either Generate short-term API key or Generate long-term API key 4.For long-term keys, set your desired expiration time and optionally configure advanced permissions 5.Choose Generate and copy your API key Set Your API Key as Environment Variable\nYou can set your API key as an environment variable so that it’s automatically recognized when you make API requests:\n# To set the API key as an environment variable, you can open a terminal and run the following command: export AWS_BEARER_TOKEN_BEDROCK=\u0026lt;YOUR API KEY HERE\u0026gt; --- The Boto3 and AWS JavaScript SDKs automatically detect your environment variable when you create an Amazon Bedrock client. Make sure you use the latest SDK version.\nMake Your First API Call You can now make API calls to Amazon Bedrock in multiple ways: 1.Using curl\ncurl -X POST \u0026#34;https://bedrock-runtime.us-east-1.amazonaws.com/model/us.anthropic.claude-3-5-haiku-20241022-v1:0/converse\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -H \u0026#34;Authorization: Bearer $AWS_BEARER_TOKEN_BEDROCK\u0026#34; \\ -d \u0026#39;{ \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [{\u0026#34;text\u0026#34;: \u0026#34;Hello\u0026#34;}] } ] }\u0026#39; 2.Using the Boto3 SDK for Amazon Bedrock:\nimport boto3 # Create an Amazon Bedrock client client = boto3.client( service_name=\u0026#34;bedrock-runtime\u0026#34;, region_name=\u0026#34;us-east-1\u0026#34; # If you\u0026#39;ve configured a default region, you can omit this line ) # Define the model and message model_id = \u0026#34;us.anthropic.claude-3-5-haiku-20241022-v1:0\u0026#34; messages = [{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [{\u0026#34;text\u0026#34;: \u0026#34;Hello\u0026#34;}]}] response = client.converse( modelId=model_id, messages=messages, ) # Print the response print(response[\u0026#39;output\u0026#39;][\u0026#39;message\u0026#39;][\u0026#39;content\u0026#39;][0][\u0026#39;text\u0026#39;]) 3.You can also use native libraries like Python Requests:\nimport requests import os url = \u0026#34;https://bedrock-runtime.us-east-1.amazonaws.com/model/us.anthropic.claude-3-5-haiku-20241022-v1:0/converse\u0026#34; payload = { \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [{\u0026#34;text\u0026#34;: \u0026#34;Hello\u0026#34;}] } ] } headers = { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Authorization\u0026#34;: f\u0026#34;Bearer {os.environ[\u0026#39;AWS_BEARER_TOKEN_BEDROCK\u0026#39;]}\u0026#34; } response = requests.request(\u0026#34;POST\u0026#34;, url, json=payload, headers=headers) print(response.text) Bridging developer experience and enterprise security requirements As an administrator, you can enable short-term API keys to streamline user onboarding for Amazon Bedrock foundation models while ensuring a higher level of security. These keys leverage AWS Signature Version 4 and existing IAM principals, maintaining your established access controls.\nFor audit and compliance purposes, all API calls are logged in AWS CloudTrail. API keys are passed as authorization headers to API requests and are not logged.\nControlling permissions for API keys You can use Service Control Policies (SCPs) with Amazon Bedrock condition keys to customize API key generation and usage to meet your organization’s requirements.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;bedrock:CallWithBearerToken\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } You can also enforce expiration limits on long-term API keys to ensure regular rotation. The following SCP prevents creation of keys with lifespans exceeding 30 days:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;iam:CreateServiceSpecificCredential\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;iam:ServiceSpecificCredentialServiceName\u0026#34;: \u0026#34;bedrock.amazonaws.com\u0026#34; }, \u0026#34;NumericGreaterThanEquals\u0026#34;: { \u0026#34;iam:ServiceSpecificCredentialAgeDays\u0026#34;: \u0026#34;30\u0026#34; } } } ] } Refer to the Amazon Bedrock documentation for additional SCP examples.\nConclusion Amazon Bedrock API keys can be used in the commercial AWS regions Amazon Bedrock is available. To learn more about API keys in Amazon Bedrock, visit the API Keys documentation in the Amazon Bedrock user guide.\nGive API keys a try in the Amazon Bedrock console today and send feedback to AWS re:Post for Amazon Bedrock or through your usual AWS Support contacts.\n"},{"uri":"/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"New features and developer experience with enhanced Amazon Location Service This blog post was written by Yasunori Kirimoto – CEO of MIERUNE\nBuilding geospatial applications requires expertise in handling geospatial data, as well as designing and developing systems. It also requires skills in collecting and managing vast amounts of geospatial data and using it effectively within the application. This process can be very labor intensive, but its complexity can be greatly reduced by leveraging Amazon Location Service.\nWith Amazon Location Service, highly accurate geospatial data can be quickly obtained from APIs, allowing developers to focus on building applications. In addition, Amazon Location Service has been updated, adding new features in addition to its previous functionality. We’ll introduce the new features of Amazon Location Service and demonstrate how to leverage them in your application.\nNew features released from Amazon Location Service The biggest change is that resource creation is no longer required. This means users no longer need to create individual resources (such as Place Indexes, Maps, and Route Calculators), but can set up an API key and immediately start using Amazon Location Service.\nIn addition, significant enhancements and new features have been added to the Maps, Places, and Routes APIs. The Maps API has been updated with additional styles, as well as the new static map capability. The Places API has been enhanced with new search and geocoding capabilities. Finally, the Routes API has been updated with new features such as Snap to Road, Waypoint Optimization, and additional travel modes.\nCreating API Keys In order to create an API Key, we can utilize the AWS Management Console, or the AWS Cloud Control API. For this example, we will use the console. Navigate to the Amazon Location Service Console, and select API keys under Manage resources. Select Create API key For the purposes of our demonstration, we will name the API Key LasVegasMaps and select the following actions:\n– GetStaticMap – GetTile – Geocode – GetPlace – SearchNearby – SearchText – CalculateIsolines – SnapToRoads\nScrolling down, we have additional options including the ability to set an Expire time, and Referers. These are optional, but we highly recommend them for production applications. NOTE: For the purposes of this demonstration we are leaving these as Default.\nSelect Create API key. Now with the API Key created, we need to retrieve the value to use in our application. Select Show API key value and copy the value into a safe location.\nNew Maps API features First, we will highlight and introduce the GetStyleDescriptor and GetStaticMap functions.\nBuilding the foundation for a map application with GetStyleDescriptor The GetStyleDescriptor function allows you to retrieve map style information and quickly build the foundation for your map application. This feature could be used for various geospatial solutions and application foundations. The new version offers expanded map styles, purpose-built for different applications—offering dark and light mode with varying levels of map detail.\nWe will demonstrate how to take advantage of these map styles using MapLibre GL JS. We’ll create a very straightforward HTML page using MapLibre GL JS and Amazon Location Service API Keys.\nBegin by creating a blank HTML page, naming it simpleMap.html, and copying the following code into the page:\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Amazon Location Service Map\u0026lt;/title\u0026gt; \u0026lt;!-- MapLibre GL CSS --\u0026gt; \u0026lt;link href=\u0026#34;https://unpkg.com/maplibre-gl@3.x/dist/maplibre-gl.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; /\u0026gt; \u0026lt;style\u0026gt; body { margin: 0; } #map { height: 100vh; /* Full viewport height */ } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;!-- Map container --\u0026gt; \u0026lt;div id=\u0026#34;map\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;!-- JavaScript dependencies --\u0026gt; \u0026lt;script src=\u0026#34;https://unpkg.com/maplibre-gl@3.x/dist/maplibre-gl.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; // Configuration const region = \u0026#34;\u0026lt;AWS Region\u0026gt;\u0026#34;; // Replace with your AWS region const mapApiKey = \u0026#34;\u0026lt;Your API Key\u0026gt;\u0026#34;; // Replace with your Amazon Location Service API key async function initializeMap() { // Initialize the map const map = new maplibregl.Map({ container: \u0026#34;map\u0026#34;, // HTML element ID where the map will be rendered center: [-115.1473824627421, 36.17071351509272], // Initial map center coordinates (Las Vegas) zoom: 12, // Initial zoom level style: `https://maps.geo.${region}.amazonaws.com/v2/styles/Standard/descriptor?\u0026amp;color-scheme=Light\u0026amp;variant=Default\u0026amp;key=${mapApiKey}`, // Map style URL }); // Add navigation controls to the map map.addControl(new maplibregl.NavigationControl(), \u0026#34;top-left\u0026#34;); } // Call the function to initialize the map initializeMap(); \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Now open this HTML page in a browser. You should see a map of Las Vegas, NV. The key to this map is the style URL we set previously in our code. In this URL, we’ve requested that we use the Standard style map in a light color scheme. We can add additional parameters such as political views as well.\nCreating a static map image with GetStaticMap zoom level, and image size you specify. This feature helps include map images in printed materials and media posts. There are various parameters for this feature, including the ability to overlay other data (such as points, lines and polygons). We’ve provided a basic example. Be certain to edit the URL for your AWS Region and your newly created API Key. Paste the following URL into the address bar of your web browser to display a static map image of the specified location: https://maps.geo..amazonaws.com/v2/static/map?center=-115.170,36.122\u0026amp;zoom=15\u0026amp;width=1024\u0026amp;height=1024\u0026amp;key=\nNew Places API features Next, we will highlight and introduce the SearchText and SearchNearby features\nSearching for specified POI data with SearchText The SearchText feature allows users to search for and present specified points of interest (POI) data. This feature is designed for users to quickly search for a specific location or facility. Users can send a POST request with the specified parameters and receive a response containing candidate point data. We will demonstrate an example of visualizing that data on a map.\nCreate a new HTML file named searchText.html and paste the following contents into the file:\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Amazon Location Service – Search Text\u0026lt;/title\u0026gt; \u0026lt;link href=\u0026#34;https://unpkg.com/maplibre-gl@3.x/dist/maplibre-gl.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; /\u0026gt; \u0026lt;style\u0026gt; body { margin: 0; } #map { height: 100vh; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;!-- map container --\u0026gt; \u0026lt;div id=\u0026#34;map\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;!-- JavaScript dependencies --\u0026gt; \u0026lt;script src=\u0026#34;https://unpkg.com/maplibre-gl@3.x/dist/maplibre-gl.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- Import the Amazon Location Client --\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/@aws/amazon-location-client@1\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- Import the utility library --\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/@aws/amazon-location-utilities-datatypes@1\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; // Configuration // Set the AWS region for Amazon Location Service const region = \u0026#34;\u0026lt;AWS Region\u0026gt;\u0026#34;; // API key for authenticating requests const mapApiKey = \u0026#34;\u0026lt;Amazon Location Service API Key\u0026gt;\u0026#34;; async function initializeMap() { // Create an authentication helper using the API key const authHelper = await amazonLocationClient.withAPIKey(mapApiKey, region); // Initialize the Amazon Location Service Places client const client = new amazonLocationClient.places.GeoPlacesClient( authHelper.getClientConfig() ); // Define search parameters for coffee shops const SearchTextInput = { BiasPosition: [-115.170, 36.122], // Las Vegas coordinates MaxResults: 25, QueryText: \u0026#34;Coffee Shops\u0026#34; } // Perform the search using Amazon Location Service const searchResults = await client.send( new amazonLocationClient.places.SearchTextCommand(SearchTextInput) ) // Initialize the map const map = new maplibregl.Map({ container: \u0026#34;map\u0026#34;, center: [-115.170, 36.122], // Las Vegas coordinates zoom: 14, style: `https://maps.geo.${region}.amazonaws.com/v2/styles/Standard/descriptor?\u0026amp;color-scheme=Light\u0026amp;variant=Default\u0026amp;key=${mapApiKey}`, }); // Add navigation controls to the map map.addControl(new maplibregl.NavigationControl(), \u0026#34;top-left\u0026#34;); // When the map is loaded, add search results as a layer map.on(\u0026#34;load\u0026#34;, () =\u0026gt; { // Convert search results into a GeoJSON FeatureCollection const featureCollection = amazonLocationDataConverter.searchTextResponseToFeatureCollection(searchResults); // Add a data source containing GeoJSON from the search results map.addSource(\u0026#34;place-index-results\u0026#34;, { type: \u0026#34;geojson\u0026#34;, data: featureCollection, }); // Add a new layer to visualize the points map.addLayer({ id: \u0026#34;place-index-results\u0026#34;, type: \u0026#34;circle\u0026#34;, source: \u0026#34;place-index-results\u0026#34;, paint: { \u0026#34;circle-radius\u0026#34;: 8, \u0026#34;circle-color\u0026#34;: \u0026#34;#0080ff\u0026#34;, }, }); // Add click event listener for the search result points map.on(\u0026#39;click\u0026#39;, \u0026#39;place-index-results\u0026#39;, (e) =\u0026gt; { if (e.features.length \u0026gt; 0) { const feature = e.features[0]; const coordinates = feature.geometry.coordinates.slice(); // Create a formatted HTML string with the feature\u0026#39;s properties const properties = feature.properties; let description = \u0026#39;\u0026lt;h3\u0026gt;\u0026#39; + (properties[\u0026#39;Title\u0026#39;] || \u0026#39;Unnamed Location\u0026#39;) + \u0026#39;\u0026lt;/h3\u0026gt;\u0026#39;; description += \u0026#39;\u0026lt;p\u0026gt;Address: \u0026#39; + (properties[\u0026#39;Address.Label\u0026#39;] || \u0026#39;N/A\u0026#39;) + \u0026#39;\u0026lt;/p\u0026gt;\u0026#39;; // Create and display a popup with the location information new maplibregl.Popup() .setLngLat(coordinates) .setHTML(description) .addTo(map); } }); map.on(\u0026#39;mouseenter\u0026#39;, \u0026#39;place-index-results\u0026#39;, () =\u0026gt; { map.getCanvas().style.cursor = \u0026#39;pointer\u0026#39;; }); map.on(\u0026#39;mouseleave\u0026#39;, \u0026#39;place-index-results\u0026#39;, () =\u0026gt; { map.getCanvas().style.cursor = \u0026#39;\u0026#39;; }); }); } // Call the function to initialize the map initializeMap(); \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Opening the HTML file in a browser will show the following map with coffee shops centered around the Venetian Resort in Las Vegas, NV.\nSearching to retrieve POI data around a specified location with SearchNearby The SearchNearby function allows you to retrieve POI data near a specified location. This feature is handy for users to search for nearby stores and attractions. Users can send a POST request with the specified parameters and receive a response containing candidate point data. We will demonstrate an example of visualizing that data on a map.\nCreate a new HTML file named searchNearby.html and paste the following contents into the file:\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Amazon Location Service – Search Nearby\u0026lt;/title\u0026gt; \u0026lt;link href=\u0026#34;https://unpkg.com/maplibre-gl@3.x/dist/maplibre-gl.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; /\u0026gt; \u0026lt;style\u0026gt; body { margin: 0; } #map { height: 100vh; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;!-- map container --\u0026gt; \u0026lt;div id=\u0026#34;map\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;!-- JavaScript dependencies --\u0026gt; \u0026lt;script src=\u0026#34;https://unpkg.com/maplibre-gl@3.x/dist/maplibre-gl.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- Import the Amazon Location Client --\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/@aws/amazon-location-client@1\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- Import the utility library --\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/@aws/amazon-location-utilities-datatypes@1\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; // Configuration // Set the AWS region for Amazon Location Service const region = \u0026#34;\u0026lt;AWS Region\u0026gt;\u0026#34;; // API key for authenticating map requests const mapApiKey = \u0026#34;\u0026lt;Amazon Location Service API Key\u0026gt;\u0026#34;; async function initializeMap() { // Create an authentication helper using the API key const authHelper = await amazonLocationClient.withAPIKey(mapApiKey, region); // Initialize the Amazon Location Service Places client const client = new amazonLocationClient.places.GeoPlacesClient( authHelper.getClientConfig() ); // Define search parameters for nearby casinos and hotels const SearchNearbyInput = { QueryPosition: [-115.170, 36.122], // Las Vegas coordinates MaxResults: 25, Filter: { IncludeCategories: [ \u0026#34;casino\u0026#34;, \u0026#34;hotel\u0026#34; ] } } // Perform the nearby search using Amazon Location Service const searchResults = await client.send( new amazonLocationClient.places.SearchNearbyCommand(SearchNearbyInput) ) // Initialize the map const map = new maplibregl.Map({ container: \u0026#34;map\u0026#34;, center: [-115.170, 36.122], // Las Vegas coordinates zoom: 15, style: `https://maps.geo.${region}.amazonaws.com/v2/styles/Standard/descriptor?\u0026amp;color-scheme=Light\u0026amp;variant=Default\u0026amp;key=${mapApiKey}`, }); // Add navigation controls to the map map.addControl(new maplibregl.NavigationControl(), \u0026#34;top-left\u0026#34;); // When the map is loaded, add search results as a layer map.on(\u0026#34;load\u0026#34;, () =\u0026gt; { // Convert search results into a GeoJSON FeatureCollection const featureCollection = amazonLocationDataConverter.searchNearbyResponseToFeatureCollection(searchResults); // Add a data source containing GeoJSON from the search results map.addSource(\u0026#34;place-index-results\u0026#34;, { type: \u0026#34;geojson\u0026#34;, data: featureCollection, }); // Add a new layer to visualize the points map.addLayer({ id: \u0026#34;place-index-results\u0026#34;, type: \u0026#34;circle\u0026#34;, source: \u0026#34;place-index-results\u0026#34;, paint: { \u0026#34;circle-radius\u0026#34;: 8, \u0026#34;circle-color\u0026#34;: \u0026#34;#0080ff\u0026#34;, }, }); // Add click event listener for the search result points map.on(\u0026#39;click\u0026#39;, \u0026#39;place-index-results\u0026#39;, (e) =\u0026gt; { if (e.features.length \u0026gt; 0) { const feature = e.features[0]; const coordinates = feature.geometry.coordinates.slice(); // Create a formatted HTML string with the feature\u0026#39;s properties const properties = feature.properties; let description = \u0026#39;\u0026lt;h3\u0026gt;\u0026#39; + (properties[\u0026#39;Title\u0026#39;] || \u0026#39;Unnamed Location\u0026#39;) + \u0026#39;\u0026lt;/h3\u0026gt;\u0026#39;; description += \u0026#39;\u0026lt;p\u0026gt;Address: \u0026#39; + (properties[\u0026#39;Address.Label\u0026#39;] || \u0026#39;N/A\u0026#39;) + \u0026#39;\u0026lt;/p\u0026gt;\u0026#39;; // Create and display a popup with the location information new maplibregl.Popup() .setLngLat(coordinates) .setHTML(description) .addTo(map); } }); map.on(\u0026#39;mouseenter\u0026#39;, \u0026#39;place-index-results\u0026#39;, () =\u0026gt; { map.getCanvas().style.cursor = \u0026#39;pointer\u0026#39;; }); map.on(\u0026#39;mouseleave\u0026#39;, \u0026#39;place-index-results\u0026#39;, () =\u0026gt; { map.getCanvas().style.cursor = \u0026#39;\u0026#39;; }); }); } // Call the function to initialize the map initializeMap(); \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Opening the HTML file in a browser will show the following map showing hotel and casino locations centered around the Venetian Resort in Las Vegas, NV.\nNew Routes API features Finally, we will discuss the new CalculateIsolines and SnapToRoads functions.\nFind the reachable range from a specified location with CalculateIsolines The CalculateIsolines function can retrieve the reachable range from a specified point. Some use cases for Isolines include identifying deliverable areas and evaluating property locations. Users can send a POST request with the specified parameters and receive a response containing polygon data indicating the reachable area. We will demonstrate an example of visualizing that data on a map.\nCreate a new HTML file and name it calculateIsolines.html and paste the following contents into the file:\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Amazon Location Service - Isolines\u0026lt;/title\u0026gt; \u0026lt;link href=\u0026#34;https://unpkg.com/maplibre-gl@3.x/dist/maplibre-gl.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; /\u0026gt; \u0026lt;style\u0026gt; body { margin: 0; } #map { height: 100vh; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;!-- map container --\u0026gt; \u0026lt;div id=\u0026#34;map\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;!-- JavaScript dependencies --\u0026gt; \u0026lt;script src=\u0026#34;https://unpkg.com/maplibre-gl@3.x/dist/maplibre-gl.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- Import the Amazon Location Client --\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/@aws/amazon-location-client@1\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- Import the utility library --\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/@aws/amazon-location-utilities-datatypes@1\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; // Configuration // Set the AWS region for the Amazon Location Service const region = \u0026#34;\u0026lt;AWS Region\u0026gt;\u0026#34;; // API key for authenticating map requests const mapApiKey = \u0026#34;\u0026lt;Amazon Location Service API Key\u0026#34;; async function initializeMap() { // Create an authentication helper using the API key const authHelper = await amazonLocationClient.withAPIKey(mapApiKey, region); // Initialize the Amazon Location Service Routes client const client = new amazonLocationClient.routes.GeoRoutesClient( authHelper.getClientConfig() ); // Define parameters for calculating isolines const IsolinesInput = { Origin: [-115.17015436843275, 36.12122662193694], // Starting point coordinates Thresholds: { Time: [ 300, 600, 900 // Time thresholds in seconds ] }, TravelMode: \u0026#34;Pedestrian\u0026#34; // Travel mode for isoline calculation } // Calculate isolines using Amazon Location Service const routeResults = await client.send( new amazonLocationClient.routes.CalculateIsolinesCommand(IsolinesInput) ) // Initialize the map const map = new maplibregl.Map({ container: \u0026#34;map\u0026#34;, center: [-115.16766776735061, 36.12177195550658], // Map center coordinates zoom: 15, style: `https://maps.geo.${region}.amazonaws.com/v2/styles/Standard/descriptor?\u0026amp;color-scheme=Light\u0026amp;variant=Default\u0026amp;key=${mapApiKey}`, }); // Add navigation controls to the map map.addControl(new maplibregl.NavigationControl(), \u0026#34;top-left\u0026#34;); // Add a marker at the origin point const marker = new maplibregl.Marker() .setLngLat([-115.17015436843275, 36.12122662193694]) .addTo(map) // When the map is loaded, add isolines as layers map.on(\u0026#34;load\u0026#34;, () =\u0026gt; { // Convert isoline results into a GeoJSON FeatureCollection const featureCollection = amazonLocationDataConverter.calculateIsolinesResponseToFeatureCollection(routeResults); // Add a data source containing GeoJSON from the isoline results map.addSource(\u0026#34;isolines\u0026#34;, { type: \u0026#34;geojson\u0026#34;, data: featureCollection }); // Add a fill layer to visualize the isoline areas map.addLayer({ \u0026#39;id\u0026#39;: \u0026#39;isolines-fill-900\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;fill\u0026#39;, \u0026#39;source\u0026#39;: \u0026#39;isolines\u0026#39;, \u0026#39;filter\u0026#39;: [\u0026#39;==\u0026#39;, [\u0026#39;get\u0026#39;, \u0026#39;TimeThreshold\u0026#39;], 900], \u0026#39;paint\u0026#39;: { \u0026#39;fill-color\u0026#39;: \u0026#39;#0000ff\u0026#39;, \u0026#39;fill-opacity\u0026#39;: 0.5 } }); // Add a layer for 600m (10) map.addLayer({ \u0026#39;id\u0026#39;: \u0026#39;isolines-fill-600\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;fill\u0026#39;, \u0026#39;source\u0026#39;: \u0026#39;isolines\u0026#39;, \u0026#39;filter\u0026#39;: [\u0026#39;==\u0026#39;, [\u0026#39;get\u0026#39;, \u0026#39;TimeThreshold\u0026#39;], 600], \u0026#39;paint\u0026#39;: { \u0026#39;fill-color\u0026#39;: \u0026#39;#00ff00\u0026#39;, \u0026#39;fill-opacity\u0026#39;: .5 } }); // Add a layer for 300m (5) map.addLayer({ \u0026#39;id\u0026#39;: \u0026#39;isolines-fill-300\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;fill\u0026#39;, \u0026#39;source\u0026#39;: \u0026#39;isolines\u0026#39;, \u0026#39;filter\u0026#39;: [\u0026#39;==\u0026#39;, [\u0026#39;get\u0026#39;, \u0026#39;TimeThreshold\u0026#39;], 300], \u0026#39;paint\u0026#39;: { \u0026#39;fill-color\u0026#39;: \u0026#39;#f10000\u0026#39;, \u0026#39;fill-opacity\u0026#39;: 0.5 } }); // Add an outline layer to highlight the isoline boundaries map.addLayer({ \u0026#39;id\u0026#39;: \u0026#39;isolines-outline\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;line\u0026#39;, \u0026#39;source\u0026#39;: \u0026#39;isolines\u0026#39;, \u0026#39;paint\u0026#39;: { \u0026#39;line-color\u0026#39;: \u0026#39;#000000\u0026#39;, \u0026#39;line-width\u0026#39;: 2 } }); }); } // Call the function to initialize the map initializeMap(); \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Opening the HTML file in a browser will show the following map (Figure 9). This map shows walkable distances from the Venetian Resort in 5-, 15-, and 30-minute timeframes.\nObtaining location-corrected route data with SnapToRoads The SnapToRoads function allows you to snap GPS data and other location data to the nearest road and obtain line data after location correction. This feature is very useful in improving the accuracy of vehicle tracking and traffic analysis. Users can send a POST request with specified parameters and receive a response containing position-corrected line data. We will demonstrate an example of visualizing the data before and after processing on a map.\nCreate a new HTML file named snapToRoad.html and paste the following contents into the file:\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Amazon Location Service - Snap to Roads\u0026lt;/title\u0026gt; \u0026lt;!-- MapLibre GL CSS --\u0026gt; \u0026lt;link href=\u0026#34;https://unpkg.com/maplibre-gl@3.x/dist/maplibre-gl.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; /\u0026gt; \u0026lt;style\u0026gt; body { margin: 0; } #map { height: 100vh; /* Full viewport height */ } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;!-- Map container --\u0026gt; \u0026lt;div id=\u0026#34;map\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;!-- JavaScript dependencies --\u0026gt; \u0026lt;script src=\u0026#34;https://unpkg.com/maplibre-gl@3.x/dist/maplibre-gl.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- Amazon Location Client --\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/@aws/amazon-location-client@1\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- Amazon Location utility library --\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/@aws/amazon-location-utilities-datatypes@1\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; // Configuration const region = \u0026#34;\u0026lt;AWS Region\u0026gt;\u0026#34;; // Replace with your AWS region const mapApiKey = \u0026#34;\u0026lt;Amazon Location Service API Key\u0026gt;\u0026#34;; // Replace with your API key async function initializeMap() { // Create authentication helper const authHelper = await amazonLocationClient.withAPIKey(mapApiKey, region); // Initialize Amazon Location Service Routes client const client = new amazonLocationClient.routes.GeoRoutesClient( authHelper.getClientConfig() ); // GPS trace coordinates const gpsTraceCoordinates = [ [-115.14564318728544, 36.09359703860663], [-115.14522142988834, 36.09368914760097], [-115.14477687479442, 36.09345887491297], [-115.14452610012599, 36.093560194978636], [-115.14432092085157, 36.09364309311755], [-115.14383077036334, 36.09360624951118], [-115.14303285096376, 36.09360624951118], [-115.14273648090104, 36.09375362383305], [-115.14218933616989, 36.09353256224662], [-115.14163079259018, 36.0936154604141], [-115.14126602943631, 36.093633882217304], [-115.14136861907319, 36.09338518751021], [-115.14126602943631, 36.093035171404196], [-115.14093546282783, 36.092906217708844], [-115.140479508885, 36.09289700672278], [-115.14033132385379, 36.09270357576062], [-115.13951060675709, 36.09278647480254], [-115.13900905742025, 36.09291542869438], [-115.13839351959763, 36.09298911653757], [-115.1378349760179, 36.092906217708844], [-115.13733342668108, 36.09301674946067], [-115.13703705661834, 36.09286016276663] ]; // Format trace points for Snap to Roads API const tracePoints = gpsTraceCoordinates.map(coord =\u0026gt; ({ Position: coord, })); // Snap to Roads API input const SnapInput = { TracePoints: tracePoints }; // Call Snap to Roads API const snapResults = await client.send( new amazonLocationClient.routes.SnapToRoadsCommand(SnapInput) ); // Initialize the map const map = new maplibregl.Map({ container: \u0026#34;map\u0026#34;, center: [-115.14127503567818, 36.09249839687936], // Las Vegas area zoom: 16, style: `https://maps.geo.${region}.amazonaws.com/v2/styles/Standard/descriptor?\u0026amp;color-scheme=Light\u0026amp;variant=Default\u0026amp;key=${mapApiKey}`, }); // Add navigation controls map.addControl(new maplibregl.NavigationControl(), \u0026#34;top-left\u0026#34;); // When the map is loaded, add the GPS trace and snapped route map.on(\u0026#34;load\u0026#34;, () =\u0026gt; { // Convert snap results to GeoJSON const featureCollection = amazonLocationDataConverter.snapToRoadsResponseToFeatureCollection(snapResults); // Add GPS trace source map.addSource(\u0026#34;gpsTrace\u0026#34;, { type: \u0026#34;geojson\u0026#34;, data: { type: \u0026#34;Feature\u0026#34;, geometry: { type: \u0026#34;LineString\u0026#34;, coordinates: gpsTraceCoordinates } } }); // Add snapped trace source map.addSource(\u0026#34;snappedTrace\u0026#34;, { type: \u0026#34;geojson\u0026#34;, data: featureCollection }); // Add GPS trace layer map.addLayer({ \u0026#39;id\u0026#39;: \u0026#39;gpsTrace\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;line\u0026#39;, \u0026#39;source\u0026#39;: \u0026#39;gpsTrace\u0026#39;, layout: { \u0026#34;line-join\u0026#34;: \u0026#34;round\u0026#34;, \u0026#34;line-cap\u0026#34;: \u0026#34;round\u0026#34;, }, paint: { \u0026#34;line-color\u0026#34;: \u0026#34;#00b0ff\u0026#34;, \u0026#34;line-width\u0026#34;: 8, } }); // Add snapped trace layer map.addLayer({ \u0026#39;id\u0026#39;: \u0026#39;snappedTrace\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;line\u0026#39;, \u0026#39;source\u0026#39;: \u0026#39;snappedTrace\u0026#39;, layout: { \u0026#34;line-join\u0026#34;: \u0026#34;round\u0026#34;, \u0026#34;line-cap\u0026#34;: \u0026#34;round\u0026#34;, }, paint: { \u0026#34;line-color\u0026#34;: \u0026#34;#d59a9a\u0026#34;, \u0026#34;line-width\u0026#34;: 8, } }); }); } // Initialize the map initializeMap(); \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Opening the HTML file in a browser will show the following map (Figure 10), with the blue line representing a GPS trace, and the red line representing a version that is snapped to roads.\nCleanup The only Amazon Location Service resources created during this demonstration was an API Key. To delete the API Key, navigate to the Amazon Location Service Console, select the API Key we created, and select Deactivate. Confirm your decision to deactivate the key, then select Delete. Also select that you would like to bypass the standard 90-day deactivation period.\nConclusion Amazon Location Service now offers even greater flexibility with the new features. With this update, the previously required resource creation procedure is no longer necessary, and various APIs can be used by setting an API key. This allows users to quickly and smoothly build geospatial applications.\nNotable new features include GetStyleDescriptor and GetStaticMap in the Maps API, SearchText and SearchNearby in the Places API, and CalculateIsolines and SnapToRoads in the Routes API.\nIn the Maps API, GetStyleDescriptor can be used to retrieve various map styles and apply them to your application, and GetStaticMap can generate static map images based on the coordinates and zoom levels you specify. The Places API allows you to search POI data using SearchText, and SearchNearby will enable you to find POIs around a specific location. The Routes API can use CalculateIsolines to calculate reachability from a specified point and SnapToRoads to correct GPS data to obtain accurate route data.\nThese new features allow application developers to more effectively utilize geospatial data and significantly improve the user experience. Contact an AWS Representative for more information about how we can help accelerate your business.\n"},{"uri":"/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Accelerate VMware to AWS Migration with Trianz’s Rapid Migration Offer and Concierto platform by Renuka Krishnan and Ciji Joseph on 31 OCT 2024 in AWS for VMware, Hybrid Cloud Management, Migration, Migration Acceleration Program (MAP), Migration Solutions, Partner solutions, VMware Cloud on AWS, Windows on AWS\nIntroduction In this blog post, we explore how Trianz’s Rapid Migration Offer (RMO) in collaboration with Amazon Web Services (AWS) facilitates seamless and efficient enterprise cloud adoption by leveraging the Concierto platform as the key enabler in streamlining and automating the entire migration process.\nIn the ever-evolving world of cloud computing, enterprises are constantly seeking efficient and reliable solutions to streamline their cloud adoption strategies. The VMware ecosystem has long been a cornerstone in virtualization and cloud infrastructure. With the Broadcom acquisition and increase in licensing costs, enterprises find themselves at a critical juncture to migrate and modernize their IT infrastructure swiftly and securely.\nTraditional migration methods are complex, resource intensive, and fraught with uncertainty leaving organizations grappling with lengthy timelines, cost overruns, and operational disruptions. This is where fixed-price and prescriptive migration methodologies such as RMO, come into play. By providing price predictability, expert skill set, and a proven methodology for a time-bound migration, supported by automations and tooling that can orchestrate the entire end-to-end migration journey, these solutions help simplify and accelerate the cloud journey.\nChallenges in VMware migration through traditional solutions Traditional migration methods require deep technical expertise, increasing errors and delays. Reliance on complex scripting and command-line tools can overwhelm IT professionals lacking coding skills. Limited automation necessitates manual intervention, amplifying human error. Inadequate testing capabilities and resource-intensive manual processes can lead to cost overruns and higher staffing demands. Lack of automated discovery and assessment tools in managing dependencies between applications, databases, and storage further adds complexity and error risk to the migration process. These challenges emphasize the need for a more streamlined, prescriptive, and automated approach to facilitate smoother transitions to the AWS Cloud.\nRapid Migration Offer Large-scale cloud migrations often face lengthy timelines and cost overruns due to a lack of specialized skills and tools, requiring development, hiring, or outsourcing. Migration projects are often structured on a “Time and Materials” (T\u0026amp;M) basis, leaving customers with uncertain costs and timelines, bearing the burden of delays and overruns.\nTo address these issues, the AWS Migration Acceleration Program (MAP) team created the Rapid Migration Offer (RMO), a prescriptive, fixed-price migration methodology designed to de-risk and accelerate enterprise-wide migrations to AWS. RMO offers several key benefits. 1.Cost transparency – Unlike traditional time and materials (T\u0026amp;M) projects, RMO clearly lays out the customer’s migration price-per-server upfront, providing cost predictability. 2. Simplified migration strategy – RMO provides an “easy button” for lift-and-shift portions of a migration, allowing customers to focus on modernization efforts. 3. Turnkey migration delivery – RMO is a prescriptive, end-to-end offering where consulting partners or AWS ProServe handle the entire migration process, including onboarding customers onto a managed services provider (MSP), if needed.\nTrianz has collaborated with AWS to deliver RMO as a comprehensive solution for accelerating enterprise-wide migrations to AWS. By leveraging Concierto, a hyper automated zero code SaaS platform, in conjunction with RMO’s prescriptive methodology, organizations can significantly reduce the migration cycle time. For example, Concierto recently completed a large-scale VMware and Windows migration for a customer, involving more than 1,400 VMs within 4 months, encompassing rapid discovery, automated assessment, and migration execution using the RMO framework.\nTrianz Approach to RMO and Cloud Migration Trianz’s fixed-price RMO offering leverages a team of skilled migration practitioners, the proven migration methodology that utilizes AWS migration best practices, and Concierto’s migration solution (Concierto Migrate) to deliver a highly automated and accelerated path to the cloud, across three key phases.\nAssess and mobilize This phase accelerates the discovery, assessment, and planning process from months to few days. The comprehensive approach minimizes risks, optimizes strategies, and ensures efficient and cost-effective cloud adoption for enterprises.\nAutomated assessment – The migration process starts with comprehensive IT landscape discovery, identifying all assets and dependencies. The platform automatically generates detailed assessments of IT infrastructure and applications, identifying gaps using the AWS Cloud Adoption Framework (Figure 1). This provides target cloud recommendations, costs, and licensing implications to support informed decision-making.\n*** Migration wave planning** – Migration tasks are grouped by related applications and infrastructure to create migration waves based on business priorities and dependencies. The platform intelligently creates the move groups based on application dependencies, enabling sequencing, effort estimation, and timeline scheduling for a clear execution plan (Figure 2).\nConcierto integrates landing zone creation, configuring the target cloud environment with necessary account structures, network designs, and security controls 2.Migrate and modernize This phase enables migrations with minimal disruptions, with clear visibility and control. Automated workflows slash migration time, reduce errors, and optimize post-migration performance and cost. Concierto enables scheduled migrations with near-zero downtime and continuous data sync, allowing real-time monitoring and adjustments through migration dashboards, ensuring control over every migration phase 3.Operate and optimize The operate and optimize phase helps maximize cloud investments by balancing cost-effectiveness and high performance. This includes monitoring and optimizing expenditures to ensure the AWS infrastructure is cost-effective and high performing\nWhat you get out of this Accelerated AWS migration – Accelerate bulk migrations to the cloud by 40% or more through pre-built catalogs and simplified workflows. Hybrid cloud automation and orchestration – Manage cloud and on-premises operations through a single platform with hundreds of built-in automations. Consolidated hybrid cloud operations management – Centralized lifecycle management through a user-friendly interface, eliminating the need for legacy software and reducing complexity. Reduced total cost of ownership (TCO) – Up to 30% lower TCO through streamlined processes and increased automation.\nStrategic programs and offerings In collaboration with AWS, Concierto offers a suite of programs and offerings for customers:\nFree combined assess and mobilize Zero upfront cost migrations with the Concierto MIGRATE Fixed price (per VM) Concierto MANAGE for hybrid cloud operations. Net-zero dollar licensing through the AWS ISV Migration Tooling Offer. Conclusion In this blog post, we explored how the collaboration between AWS and Trianz offers a reliable, efficient, and simplified pathway for organizations to optimize their IT investments amid the evolving VMware landscape. The Trianz RMO leveraging Concierto addresses migration challenges by removing complexity, preventing cost overruns, and minimizing manual errors. The unified platform simplifies operations and streamlines dependency management, centralizing control to enhance visibility and enable smoother transitions with optimized performance.\nAWS has significantly more services, and more features within those services, than any other cloud provider, making it faster, easier, and more cost effective to move your existing applications to the cloud and build nearly anything you can imagine. Give your Microsoft applications the infrastructure they need to drive the business outcomes you want. Visit our .NET on AWS and AWS Database blogs for additional guidance and options for your Microsoft workloads. Contact us to start your migration and modernization journey today.\n"},{"uri":"/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"AI-DRIVEN DEVELOPMENT WORKSHOP — Shaping the Future of Development** Location: AWS Event Hall, 26th Floor — Bitexco Tower, Ho Chi Minh City\nTime: 14:00 – 16:30, Friday, 3 October 2025\nSpeakers: Toan Huynh, My Nguyen\nOrganizers: Diem My, Dai Truong, Dinh Nguyen\nEvent objectives Share new trends in AI-driven software development (AI-Driven Development). Present the AI-Driven Development Lifecycle (AI-DLC) — a model integrating AI across the software lifecycle. Demo two important tools: Amazon Q Developer and Kiro IDE extension. Analyze how AI improves productivity, speed and product quality. AI-DLC: three evolution stages AI-Assisted Development: AI supports code generation, suggestions and syntax checks. AI-Driven Development: AI participates in architecture, planning and decision support. AI-Managed Development: AI orchestrates parts of the delivery pipeline with human approval. In this model AI acts as a smart coordinator, while humans retain final decision and validation responsibilities.\nBenefits of AI in software development Predictability: Maintain schedules and better forecast release timelines. Velocity: Accelerate time-to-market for ideas. Quality: Reduce defects and increase stability. Innovation: Enable idea exploration and new approaches. Developer engagement: Improve developer satisfaction and effectiveness. Customer satisfaction: Enhance user experience and trust. Productivity: Reduce development time and increase overall throughput. Standard AI‑DLC workflow The workshop presented a four-step workflow:\nRequirement: Product Owner gathers and analyzes requirements. Design: Software Architect defines system design and APIs. Implementation: Software Engineers develop, test and integrate. Deployment: Release and monitor the system. AI supports all phases to make them more consistent, traceable and efficient.\nKey workflow features Role separation: Clear separation between product, architecture and implementation. AI‑Enhanced: Each role has AI personas tailored to the responsibility. Iterative: Continuous feedback loops between stages. Template‑driven: Standardized outputs using AI‑DLC templates. AI in each development stage Specific context: Roles (PM, Architect, Developer) get role‑specific AI assistants. Clear inputs/outputs: Define precise inputs and expected outputs between steps. Interactive approach: Human-AI collaboration with real-time feedback. Documentation: Keep prompts.md and dashboard.md updated to track progress and improvements. Demos: Amazon Q Developer \u0026amp; Kiro IDE Amazon Q Developer\nAI assistant integrated into IDEs (VS Code, Cloud9, \u0026hellip;). Automates code generation, testing, documentation and AWS architecture suggestions. Helps update prompt.md and automates parts of CI/CD. Demo showed planning, generating user stories and managing project tasks via AI. Kiro IDE (demo by My Nguyen)\nExtension for generating and managing spec documents (requirements.md, design.md, tasks.md). AI can scaffold feature descriptions, API flows and backend code. Demo included building a Chat app with AI-driven auth flows (signup, login, token handling). Lessons learned AI complements developers as a “smart teammate”, not a replacement. AI-DLC provides consistency and transparency across the lifecycle. Amazon Q Developer saves time, reduces defects, and helps automate testing and deployment. Kiro IDE demonstrates end-to-end support from spec to code. Integrating AI into DevSecOps is essential for high productivity and secure delivery. Practical applications Use Amazon Q Developer to automate internal test generation and documentation. Use Kiro IDE to standardize spec workflows and speed backend development. Run AI‑driven sprints to evaluate AI impact on team delivery. Adopt “AI-Assisted Code Review” to improve product quality. Personal impressions The “AI-Driven Development Workshop” was a focused, practical session that clarified how AI can be embedded across the software lifecycle. Presentations by Toan Huynh and My Nguyen provided actionable guidance and real demos that are directly applicable to our projects.\n"},{"uri":"/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Luong Thanh Huy\nPhone Number: 0932198848\nEmail: huyltse184712@fpt.edu.vn\nUniversity: FPT University HCMC\nMajor: Sofware Engineer\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 00/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"Workshop Overview — LearningHub (2HTD) This workshop guides participants to deploy and validate the core components of 2HTD-LearningHub as described in the proposal: frontend hosted on Vercel, backend serverless with AWS Lambda + API Gateway, relational data on SQL Server running in EC2 (private subnet), and media stored in S3. The workshop focuses on hands-on tasks: provisioning network (VPC/subnets), configuring EC2 + SQL Server, setting up S3 uploads with presigned URLs, deploying Lambda + API Gateway, configuring Cognito, and accessing EC2 securely via AWS Systems Manager (SSM).\nThe workshop uses a simple test environment (optionally two VPCs: one cloud VPC for LearningHub resources and an on-prem simulation VPC) to demonstrate network flows and secure access.\nFigure: Workshop diagram — VPC, EC2 (SQL Server), S3, Lambda, API Gateway, Cognito, and SSM\nLearning Objectives Understand LearningHub\u0026rsquo;s hybrid architecture (Vercel + Lambda + EC2 + S3). Provision VPCs, subnets and configure NAT / endpoints to optimize egress cost. Deploy EC2 Windows + SQL Server Express and perform basic administration via SSM. Configure an S3 bucket and perform uploads using presigned URLs from the frontend. Deploy a Lambda (Node.js) and configure API Gateway (HTTP API) to invoke it. Configure Amazon Cognito User Pool and test authenticated API flows. Core Labs Basic provisioning: create a VPC with public/private subnets, an Internet Gateway and NAT (or NAT instance for labs). Launch an EC2 Windows instance, install SQL Server Express (or use a prebuilt AMI), and configure security groups for private access. Configure SSM Agent and access EC2 with Session Manager (no public RDP). Create a test S3 bucket, attach an IAM role to Lambda, and test uploads with presigned URLs. Deploy a simple Lambda function (Node.js) and wire it to API Gateway (HTTP API). Configure a Cognito User Pool, register a test user, and call protected APIs. Observability: enable CloudWatch Logs for Lambda, inspect logs, and create a basic dashboard. References Architecture diagram: /images/architecture.png (proposal details) Sample code for presigned URL, Lambda handler, and Terraform/CDK snippets will be provided in the workshop repo. If you prefer, I can condense the labs into 4 main exercises, or expand each lab into a step-by-step checklist (commands, IAM policies, Terraform snippets).\n"},{"uri":"/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 — Goals Introduce myself and connect with the First Cloud Journey cohort. Gain a practical introduction to core AWS services and the Console vs CLI workflows. Planned activities this week Day Activity Start Date Completion Date Reference Material 2 - Meet fellow FCJ members and review internship guidelines 08/11/2025 08/11/2025 3 - Explore AWS service categories:\n+ Compute\n+ Storage\n+ Networking\n+ Database 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account\n- Install and configure AWS CLI (practice basic commands and configuration) 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Study EC2 fundamentals: instance families, AMIs, EBS volumes; learn SSH access methods and Elastic IPs 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Hands-on: launch an EC2 instance, connect via SSH, and attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 1 — Outcomes Gained a clear understanding of AWS and the main service groups:\nCompute, Storage, Networking, Database, etc. Successfully registered and configured an AWS Free Tier account.\nNavigated the AWS Management Console and located commonly used services.\nInstalled and set up AWS CLI on the workstation (configured Access Key, Secret, Region).\nUsed the AWS CLI to perform introductory checks and commands (region listing, EC2 inspection, key pair management).\nPracticed managing AWS resources using both Console and CLI workflows.\nReady to proceed to Week 2 exercises (static website + RDS / S3).\n"},{"uri":"/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 — Objectives Learn how CloudFront (CDN), DynamoDB (NoSQL) and ElastiCache (Redis) work and when to apply them. Improve website performance using CDN and explore NoSQL patterns and caching for faster responses. Planned tasks this week Day Task Start Date Completion Date Reference Material 2 - Introduction to CDN concepts and benefits of CloudFront\n- Create a CloudFront Distribution to serve S3 website content 22/09/2025 22/09/2025 AWS Journey 3 - Configure CloudFront behaviors and cache policies\n- Test site access via CloudFront URL\n- Perform invalidation to refresh cached content 23/09/2025 23/09/2025 AWS Journey 4 - Introduction to DynamoDB (NoSQL)\n- Create DynamoDB tables (Users, Products, etc.)\n- Perform CRUD operations using Console 24/09/2025 24/09/2025 AWS Journey 5 - Query and operate DynamoDB via AWS CLI\n- Write small scripts to insert and read data from tables 25/09/2025 25/09/2025 AWS Journey 6 - Explore ElastiCache (Redis \u0026amp; Memcached)\n- Create a basic Redis cluster\n- Test connecting from EC2 to read/write cached data 26/09/2025 26/09/2025 AWS Journey Week 3 — Outcomes Understood the role of a CDN (CloudFront) in improving website performance. Created and configured a CloudFront Distribution for the S3-hosted website and verified content delivery via the CDN endpoint. Created DynamoDB tables and practiced CRUD operations using Console and CLI. Deployed a simple ElastiCache Redis cluster and validated connectivity from an EC2 client. Integrated caching + NoSQL patterns into the static website model to improve access speed. "},{"uri":"/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 4 Objectives: Understand the Migration process (moving systems to AWS) and Disaster Recovery (post-incident recovery). Practice AWS Database Migration Service (DMS) and Elastic Disaster Recovery (EDR). Learn backup and restore techniques and prepare a basic emergency plan for infrastructure. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Study Migration concepts (Lift \u0026amp; Shift, Replatform, Refactor) - Introduction to AWS Database Migration Service (DMS) 29/09/2025 29/09/2025 AWS Journey 3 - Practice creating a DMS Replication Instance - Configure source (on-premises) and target (RDS) - Perform a test data migration 30/09/2025 30/09/2025 AWS Journey 4 - Introduction to Elastic Disaster Recovery (EDR) - Learn to set up replication servers and recovery instances 01/10/2025 01/10/2025 AWS Journey 5 - Perform a failover simulation: stop the primary EC2 and boot recovery instance from EDR - Evaluate recovery time objectives (RTO) and recovery point objectives (RPO) 02/10/2025 02/10/2025 AWS Journey 6 - Draft a basic Disaster Recovery plan (backup, restore, failover) - Write a summary document for the Migration + DR process - Week 4 knowledge summary 03/10/2025 03/10/2025 AWS Journey Week 4 Achievements: Understood the end-to-end Migration process for applications and databases to AWS.\nSuccessfully created and configured a DMS Replication Instance and executed a sample data migration.\nSet up Elastic Disaster Recovery (EDR) for basic protection against incidents.\nSuccessfully simulated a failover and validated recovery procedures.\nCompleted a basic Disaster Recovery plan to prepare for Week 5 (Infrastructure as Code \u0026amp; Systems Manager).\ntitle: \u0026ldquo;Week 4 Worklog\u0026rdquo; weight: 1 chapter: false pre: \u0026quot; 1.4. \u0026quot; ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 4 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 4 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Get hands-on with Infrastructure as Code (IaC) using AWS CloudFormation and AWS CDK. Manage system resources centrally with AWS Systems Manager (SSM). Understand how to automate deployment and operational tasks for AWS infrastructure. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Introduce Infrastructure as Code (IaC) concepts and benefits vs manual deployment - Get familiar with AWS CloudFormation: template, stack, parameter 06/10/2025 06/10/2025 AWS Journey 3 - Write a CloudFormation template to deploy an S3 bucket and EC2 instance - Create, update, and delete stacks via the AWS Console 07/10/2025 07/10/2025 AWS Journey 4 - Introduce AWS CDK (Cloud Development Kit) - Install AWS CDK, create a CDK project in Python or TypeScript - Write CDK code to deploy an EC2 instance 08/10/2025 08/10/2025 AWS Journey 5 - Introduce AWS Systems Manager (SSM) and core features: Parameter Store, Run Command, Automation, Session Manager - Create Parameter Store entries for configuration variables 09/10/2025 09/10/2025 AWS Journey 6 - Practice creating an Automation Document in SSM to start/stop EC2 instances - Test Session Manager (access EC2 without SSH keys) - Week summary: IaC + SSM demo 10/10/2025 10/10/2025 AWS Journey Week 5 Achievements: Understood IaC concepts and benefits for infrastructure management.\nCreated CloudFormation templates to deploy S3 \u0026amp; EC2 and successfully managed stacks.\nGot hands-on with AWS CDK and deployed resources using code.\nLearned to use AWS Systems Manager to manage configuration, run commands, and automate tasks.\n"},{"uri":"/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Understand security concepts and cost management in AWS. Practice IAM Policies, KMS, and Secrets Manager to secure resources. Monitor, analyze, and alert on usage costs using Billing \u0026amp; Cost Explorer. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review basic IAM concepts - Learn advanced IAM Policy (JSON structure, Effect, Action, Resource, Condition) - Create custom policies and attach to user/group 13/10/2025 13/10/2025 AWS Journey 3 - Introduce AWS Key Management Service (KMS) - Create a Customer Managed Key (CMK) and test file encryption/decryption - Apply KMS to encrypt S3 buckets or EBS volumes 14/10/2025 14/10/2025 AWS Journey 4 - Get familiar with AWS Secrets Manager - Create a secret to store database credentials - Write a small Lambda script to read secrets from Secrets Manager 15/10/2025 15/10/2025 AWS Journey 5 - Explore the AWS Billing Dashboard and Cost Explorer - View costs by service, region and time - Configure Cost Anomaly Detection 16/10/2025 16/10/2025 AWS Journey 6 - Create AWS Budgets and configure email alerts - Write a weekly cost summary and propose optimizations (stop unused EC2, cleanup EBS, reduce log retention, etc.) - Week 6 summary 17/10/2025 17/10/2025 AWS Journey Week 6 Achievements: Mastered creating and applying advanced IAM Policies to control resource access.\nUnderstood KMS encryption mechanisms and applied them to S3 and EBS.\nDeployed Secrets Manager to protect sensitive information, accessible from Lambda.\nMonitored costs and configured alerts with Cost Explorer and Budgets.\nLearned how to analyze costs, recommend optimizations, and maintain security in parallel.\n"},{"uri":"/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Understand and implement High Availability (HA) and Auto Scaling on AWS. Configure Elastic Load Balancers (ELB) and Auto Scaling Groups (ASG) for EC2. Use SQS/SNS for queueing and notifications. Analyze network activity using VPC Flow Logs. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Study High Availability, Fault Tolerance and Elasticity concepts - Introduction to Auto Scaling Group (ASG) and Elastic Load Balancer (ELB) 20/10/2025 20/10/2025 AWS Journey 3 - Practice creating an Auto Scaling Group for EC2 instances - Configure launch template, scaling policy and target tracking 21/10/2025 21/10/2025 AWS Journey 4 - Create and configure an Application Load Balancer (ALB) - Attach ALB to ASG for traffic distribution - Test web access through ALB DNS 22/10/2025 22/10/2025 AWS Journey 5 - Get familiar with Amazon SQS and SNS - Create SQS queue, SNS topic and subscription - Send and receive notifications between components 23/10/2025 23/10/2025 AWS Journey 6 - Enable VPC Flow Logs to monitor network traffic - Analyze logs in CloudWatch Logs - Week summary: reliability \u0026amp; scaling 24/10/2025 24/10/2025 AWS Journey Week 7 Achievements: Understood High Availability models and how to maintain uptime during incidents.\nSuccessfully deployed Auto Scaling Group + Load Balancer to automatically scale EC2.\nConfigured SQS/SNS to communicate and send notifications between AWS services.\nEnabled and read VPC Flow Logs, analyzed network traffic using CloudWatch.\nImproved system resilience, ensuring higher availability and stable performance.\n"},{"uri":"/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Review and consolidate core knowledge areas in the AWS Well-Architected Framework: security, reliability, performance, and cost optimization. Become proficient with key services: EC2, S3, IAM, RDS, VPC, Lambda, CloudWatch, CloudFront. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Overview of AWS Well-Architected Framework, 5 pillars: Operational Excellence, Security, Reliability, Performance Efficiency, Cost Optimization - Identify the role and importance of each pillar 27/10/2025 27/10/2025 AWS Journey 3 - Review Secure Architectures → IAM, MFA, SCP, Encryption (KMS, TLS/ACM), Security Groups, NACLs, GuardDuty, Shield, WAF, Secrets Manager 28/10/2025 28/10/2025 AWS Journey 4 - Review Resilient Architectures → Multi-AZ, Multi-Region, DR Strategies, Auto Scaling, Route 53, Load Balancing, Backup \u0026amp; Restore 29/10/2025 29/10/2025 AWS Journey 5 - Review High-Performing \u0026amp; Cost-Optimized Architectures → EC2 Auto Scaling, Lambda, Fargate, CloudFront, Global Accelerator, Cost Explorer, Budgets, Savings Plans, Storage Tiering 30/10/2025 30/10/2025 AWS Journey 6 - Final practical: + Build a sample architecture combining EC2, S3, RDS, IAM, VPC, CloudFront, Lambda, CloudWatch + Evaluate against the 5 Well-Architected pillars + Write weekly summary 31/10/2025 31/10/2025 AWS Journey Week 8 Achievements: Deepened understanding and systematized the AWS Well-Architected Framework.\nReinforced four core architecture focus areas: Security, Reliability, Performance, Cost Optimization.\nPracticed complete infrastructure design and self-evaluation against AWS standards.\n"},{"uri":"/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Get familiar with and practice Data \u0026amp; Analytics services on AWS. Understand the data ingestion, storage, processing and analytics pipeline on the cloud. Use AWS tools to build a Data Lake and create BI dashboards. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Introduce the Data \u0026amp; Analytics ecosystem on AWS - Understand Data Lake concepts, ETL pipelines, and how to connect data from multiple sources 03/11/2025 03/11/2025 AWS Journey 3 - Build a Data Lake on Amazon S3 - Design folder structure and access controls - Configure AWS Glue Crawler to detect data schema 04/11/2025 04/11/2025 AWS Journey 4 - Practice querying the Data Lake with AWS Athena - Write basic SQL queries and export results to S3 05/11/2025 05/11/2025 AWS Journey 5 - Introduction and hands-on with Amazon QuickSight - Connect QuickSight to Athena for visualizations - Create a simple dashboard with charts and summary tables 06/11/2025 06/11/2025 AWS Journey 6 - Review \u0026amp; consolidate the week: + Data ingestion → processing → analytics on AWS + Compare Glue, Athena, QuickSight with traditional tools + Write a summary report of exercises 07/11/2025 07/11/2025 AWS Journey Week 9 Achievements: Understood how to build and manage a Data Lake using S3.\nPracticed ingesting, cataloging and querying data with Glue and Athena.\nCreated a BI dashboard for visual analysis using QuickSight.\n"},{"uri":"/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Week 1: Getting familiar with AWS and core AWS services\nWeek 2: Build a static website with S3 and connect to an RDS database\nWeek 3: Performance optimization with CloudFront, DynamoDB and ElastiCache\nWeek 4: Migration and Disaster Recovery with DMS and EDR\nWeek 5: Infrastructure as Code with CloudFormation, CDK and Systems Manager\nWeek 6: Security and Cost Management: IAM Policy, KMS, Secrets Manager, Billing Dashboard and AWS Budgets\nWeek 7: Improve system scalability: Auto Scaling, Load Balancer, SQS/SNS, and VPC Flow Logs\nWeek 8: Review AWS Well-Architected Framework and reinforce core service knowledge\nWeek 9: Data \u0026amp; Analytics practice: Data Lake with S3, Glue, Athena and QuickSight\nWeek 10: AI/ML practice: SageMaker, Rekognition, Comprehend and Kendra\nWeek 11: Modernization \u0026amp; Serverless Architecture with Lambda, API Gateway, DynamoDB, Cognito and SAM\nWeek 12: Final project - Consolidate knowledge and build a complete AWS project\n"},{"uri":"/5-workshop/5.3-cognito-api-gateway/5.3.1-create-andconfigue-cognito/","title":"Create and configure Cognito &amp; API Gateway","tags":[],"description":"","content":"Purpose This guide shows step-by-step how to create and configure Amazon Cognito (User Pool + App Client, optional Identity Pool) and an Amazon API Gateway HTTP API that uses Cognito as a JWT authorizer. It includes console steps, AWS CLI/PowerShell commands, IAM notes and examples for testing.\nArchitecture assumption: frontend (Vercel) calls API Gateway (HTTP API) -\u0026gt; Gateway authorizer validates Cognito JWTs -\u0026gt; routes to Lambda (business logic). SQL Server runs on EC2 in a private subnet and is accessed by Lambda when needed.\n1. Create a Cognito User Pool (Console) Sign in to AWS Console -\u0026gt; Amazon Cognito -\u0026gt; \u0026ldquo;Manage User Pools\u0026rdquo; -\u0026gt; \u0026ldquo;Create a user pool\u0026rdquo;. Choose a name, e.g. learninghub-userpool. Choose Review defaults or Step through settings for custom options. Recommended custom settings: Attributes: keep email as required; add name if you need display names. Policies: password strength as required for your environment. MFA \u0026amp; verifications: Email verification enabled for workshop demos (optional). App clients: create at least one App client with no secret if used by SPA or mobile (uncheck \u0026ldquo;Generate client secret\u0026rdquo;). App client callback URLs: add your frontend URL(s) (e.g., https://learninghub.example.com) for Hosted UI flows. Save pool. Notes:\nFor SPAs use an App client without a secret. For backend-to-backend you may use a client secret. Hosted UI: you can configure a domain under \u0026ldquo;App integration -\u0026gt; Domain name\u0026rdquo; to use Cognito\u0026rsquo;s Hosted UI. 2. Create an App Client (Console + CLI) Console: In User Pool -\u0026gt; App clients -\u0026gt; \u0026ldquo;Add an app client\u0026rdquo;. Name it like learninghub-web-client. Uncheck \u0026ldquo;Generate client secret\u0026rdquo; for browser-based apps.\nAWS CLI (PowerShell):\n# create app client (no secret) aws cognito-idp create-user-pool-client --user-pool-id \u0026lt;USER_POOL_ID\u0026gt; --client-name learninghub-web-client --no-generate-secret --output json Record the ClientId and UserPoolId for later.\n3. (Optional) Configure a Hosted UI / Domain In User Pool -\u0026gt; App integration -\u0026gt; Domain name, create a Cognito domain (e.g. learninghub-demo-\u0026lt;suffix\u0026gt;). Configure callback and sign-out URLs under App client settings. Enable OAuth flows needed (Authorization code grant or Implicit). For SPAs you may use implicit (but Authorization code grant + PKCE is recommended). 4. Create a Cognito Identity Pool (optional) Use an identity pool when you need AWS temporary credentials (e.g., direct S3 access from client). Steps:\nConsole: Cognito -\u0026gt; Federated Identities -\u0026gt; Create identity pool -\u0026gt; enable \u0026ldquo;Authenticated identities\u0026rdquo; -\u0026gt; choose roles for authenticated/unauthenticated. CLI example (PowerShell):\naws cognito-identity create-identity-pool --identity-pool-name learninghub-identitypool --allow-unauthenticated-identities false --cognito-identity-providers ProviderName=cognito-idp.\u0026lt;region\u0026gt;.amazonaws.com/\u0026lt;USER_POOL_ID\u0026gt;,ClientId=\u0026lt;CLIENT_ID\u0026gt; You\u0026rsquo;ll receive an IdentityPoolId; configure IAM roles and trust policies to allow the pool to assume roles.\n5. Create IAM roles for Cognito (Identity Pool) and Lambda If you use Identity Pool, create two roles: CognitoAuthRole (for authenticated users) and CognitoUnauthRole (for unauthenticated, if enabled). Attach minimal policies (S3 GetObject/PutObject if you use direct S3 uploads).\nLambda execution role: create an IAM role that allows Lambda to access S3, Secrets Manager, EC2 (via SSM) or other services required by your backend.\nExample trust policy for a role assumed by Cognito identity pool (snippet):\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: {\u0026#34;Federated\u0026#34;: \u0026#34;cognito-identity.amazonaws.com\u0026#34;}, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRoleWithWebIdentity\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: {\u0026#34;cognito-identity.amazonaws.com:aud\u0026#34;: \u0026#34;\u0026lt;IDENTITY_POOL_ID\u0026gt;\u0026#34;}, \u0026#34;ForAnyValue:StringLike\u0026#34;: {\u0026#34;cognito-identity.amazonaws.com:amr\u0026#34;: \u0026#34;authenticated\u0026#34;} } } ] } 6. Create API Gateway (HTTP API) and configure Cognito Authorizer We use HTTP API (faster, lower-cost). Configure a JWT authorizer that validates Cognito tokens.\nConsole steps API Gateway -\u0026gt; HTTP APIs -\u0026gt; Create -\u0026gt; Build. Select \u0026ldquo;Add integration\u0026rdquo; -\u0026gt; Lambda -\u0026gt; choose your Lambda function (or create placeholder). Create routes (e.g., GET /courses, POST /exams). Attach integration. Under \u0026ldquo;Authorization\u0026rdquo; -\u0026gt; Add authorizer -\u0026gt; choose JWT. Fill these fields: Identity provider: choose Cognito. Issuer: https://cognito-idp.\u0026lt;region\u0026gt;.amazonaws.com/\u0026lt;USER_POOL_ID\u0026gt; Audience: your App Client ID (client_id) — for ID tokens use client_id as audience. Some setups use the user pool\u0026rsquo;s aud claim. Attach the JWT authorizer to routes that require authentication. Deploy (Auto-deploy for HTTP API is default). Note the invoke URL (e.g., https://\u0026lt;api-id\u0026gt;.execute-api.\u0026lt;region\u0026gt;.amazonaws.com). CLI example (create authorizer) # create an HTTP API $api=$(aws apigatewayv2 create-api --name learninghub-api --protocol-type HTTP --target arn:aws:lambda:\u0026lt;region\u0026gt;:\u0026lt;account-id\u0026gt;:function:YourFunction --output json) $apiId=$(echo $api | ConvertFrom-Json).ApiId # create JWT authorizer aws apigatewayv2 create-authorizer --api-id $apiId --authorizer-type JWT --name CognitoJWTAuth --identity-source \u0026#34;$request.header.Authorization\u0026#34; --jwt-configuration \u0026#34;{\\\u0026#34;Issuer\\\u0026#34;:\\\u0026#34;https://cognito-idp.\u0026lt;region\u0026gt;.amazonaws.com/\u0026lt;USER_POOL_ID\u0026gt;\\\u0026#34;,\\\u0026#34;Audience\\\u0026#34;:[\\\u0026#34;\u0026lt;CLIENT_ID\u0026gt;\\\u0026#34;]}\u0026#34; Attach the authorizer to a route (replace routeId as necessary):\naws apigatewayv2 update-route --api-id $apiId --route-key \u0026#34;GET /courses\u0026#34; --authorization-type JWT --authorizer-id \u0026lt;AUTHORZER_ID\u0026gt; Notes:\nHTTP API expects JWT Authorization header: Authorization: Bearer \u0026lt;id_token\u0026gt;. Use id_token for user info (claims). Access tokens may also be acceptable depending on configuration. 7. API Gateway -\u0026gt; Lambda permissions Add permission for API Gateway to invoke the Lambda:\naws lambda add-permission --function-name YourFunction --statement-id apigw-invoke --action lambda:InvokeFunction --principal apigateway.amazonaws.com --source-arn arn:aws:execute-api:\u0026lt;region\u0026gt;:\u0026lt;account-id\u0026gt;:${apiId}/*/*/* 8. CORS If your frontend is hosted on https://learninghub.example.com, add CORS to your routes:\nFor HTTP API you can configure CORS in console or set Access-Control-Allow-Origin header in integration responses. Example allowed origins: https://learninghub.example.com or * for quick demos (not recommended for production). 9. Test flow (Sign-up, Sign-in, call API) Create a test user in the User Pool (Console -\u0026gt; Users -\u0026gt; Create user) or use self-sign-up if enabled. Use Hosted UI or AWS CLI to authenticate and get tokens. CLI example (AdminInitiateAuth) to get tokens (PowerShell):\naws cognito-idp admin-initiate-auth --user-pool-id \u0026lt;USER_POOL_ID\u0026gt; --client-id \u0026lt;CLIENT_ID\u0026gt; --auth-flow ADMIN_NO_SRP_AUTH --auth-parameters USERNAME=\u0026#34;testuser\u0026#34;,PASSWORD=\u0026#34;P@ssw0rd\u0026#34; --output json Response contains AuthenticationResult.IdToken and AccessToken.\nCall API with bearer token: $token = \u0026#34;\u0026lt;ID_TOKEN_FROM_AUTH\u0026gt;\u0026#34; Invoke-RestMethod -Method Get -Uri \u0026#34;https://\u0026lt;api-id\u0026gt;.execute-api.\u0026lt;region\u0026gt;.amazonaws.com/courses\u0026#34; -Headers @{ Authorization = \u0026#34;Bearer $token\u0026#34; } If you receive 401: check authorizer configuration (issuer/audience), ensure token is not expired and your route has the authorizer attached.\n10. Troubleshooting tips 401 Unauthorized: verify Issuer URL exactly matches https://cognito-idp.\u0026lt;region\u0026gt;.amazonaws.com/\u0026lt;USER_POOL_ID\u0026gt; and Audience contains the client id used to get the token. 403 Forbidden when invoking Lambda: check Lambda resource policy and add-permission invocation. Token issues: verify token type (ID token vs Access token) and its claims using jwt.io. CORS errors: ensure API returns proper Access-Control-Allow-Origin header. 11. Security considerations Use Authorization Code Grant + PKCE for SPAs to avoid implicit flow security issues. Do not embed client secrets in browser apps. Scope Resource ARNs in IAM policies in production. Set appropriate token expiration and rotation policies. "},{"uri":"/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"Overview This page lists the prerequisites to run the workshop and to follow the hands-on exercises for the 2HTD-LearningHub project. The content below covers required AWS permissions, recommended region, AWS services used, a minimal workshop IAM policy example, and PowerShell steps for Windows users.\nAWS account \u0026amp; permissions An AWS account with permission to create and manage resources used by the workshop (CloudFormation, EC2, VPC, S3, Lambda, API Gateway, Cognito, IAM, CloudWatch, SSM). For workshops: use a sandbox account and create an IAM user for yourself. Assign AdministratorAccess if you prefer a frictionless experience during the lab. For real environments: follow the least-privilege principle. Use the minimal IAM policy below as a starting point and scope resources (ARNs) before applying in production. Quick verification after configuring the AWS CLI:\naws configure # enter Access Key, Secret, default region (e.g. us-east-1) and output format aws sts get-caller-identity # verify credentials and account Region Use us-east-1 (N. Virginia) for the workshop demos and CloudFormation examples. If you choose another region, adjust resource names and template URLs accordingly. Required AWS services Amazon EC2 (Windows + SQL Server as the project DB) Amazon S3 (media and artifact storage) AWS Lambda (backend functions) Amazon API Gateway (HTTP API) Amazon Cognito (authentication) AWS IAM (roles and policies) Amazon CloudWatch (logs \u0026amp; metrics) AWS Systems Manager (Session Manager for EC2 access) AWS CloudFormation (or Terraform/CDK) for provisioning Route 53 (optional for DNS during demos) Local tools (recommended) Git (clone repository) Node.js (LTS, e.g. 18+) and npm (build Lambda code) AWS CLI v2 (configure with aws configure) AWS Session Manager plugin (optional, to enable aws ssm start-session integration) SQL client: SQL Server Management Studio (SSMS) or Azure Data Studio (connect to SQL Server on EC2) PowerShell (Windows) or a POSIX shell (Linux/macOS) Docker (optional — only if building containerized Lambdas) Minimal IAM policy (workshop example) This example covers common actions used by provisioning templates and deployment flows in this workshop. Review and scope resource ARNs before using in production.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:CreateStack\u0026#34;, \u0026#34;cloudformation:DeleteStack\u0026#34;, \u0026#34;cloudformation:DescribeStacks\u0026#34;, \u0026#34;cloudformation:ListStacks\u0026#34;, \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;apigateway:POST\u0026#34;, \u0026#34;cognito-idp:CreateUserPool\u0026#34;, \u0026#34;cognito-idp:DeleteUserPool\u0026#34;, \u0026#34;ssm:StartSession\u0026#34;, \u0026#34;ssm:SendCommand\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Notes:\nThe policy above is suitable for a lab/sandbox account. Replace \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; with specific ARNs to narrow scope in production. Some CloudFormation templates create IAM roles and require CAPABILITY_NAMED_IAM when deploying; the iam:PassRole action is commonly necessary. PowerShell (Windows) quick-start Use the following PowerShell snippets to install the AWS CLI, configure credentials, verify access, and run common workshop commands.\nInstall AWS CLI v2 (download \u0026amp; install MSI):\n# Download installer and run Invoke-WebRequest -Uri \u0026#34;https://awscli.amazonaws.com/AWSCLIV2.msi\u0026#34; -OutFile \u0026#34;$env:TEMP\\AWSCLIV2.msi\u0026#34; Start-Process msiexec.exe -Wait -ArgumentList \u0026#34;/i $env:TEMP\\AWSCLIV2.msi /qn\u0026#34; Configure AWS CLI and verify identity:\naws configure aws sts get-caller-identity Start an SSM Session (example):\n# Replace with your EC2 instance id $instanceId = \u0026#39;i-0123456789abcdef0\u0026#39; aws ssm start-session --target $instanceId Deploy a CloudFormation template:\naws cloudformation deploy --template-file .\\cloudformation\\stack.yaml --stack-name MyWorkshopStack --capabilities CAPABILITY_NAMED_IAM Upload artifacts to S3 (example):\naws s3 mb s3://my-workshop-artifacts-$(Get-Random -Maximum 99999) aws s3 cp .\\lambda\\package.zip s3://my-workshop-artifacts-12345/ Local setup steps (recap) Install the tools listed above. Configure the AWS CLI with your IAM user: aws configure (set default region to us-east-1). Verify access: aws sts get-caller-identity. (Optional) Create an S3 bucket to store deployment artifacts. Notes on EC2 \u0026amp; SSM The project stores relational data on an EC2 instance running SQL Server in a private subnet. For administration we rely on AWS Systems Manager (SSM) Session Manager instead of RDP. Ensure your IAM user has SSM permissions (or use AdministratorAccess in a lab account). Clean-up guidance If you deploy using CloudFormation or scripts, delete the stack and any created S3 buckets when finished to avoid ongoing charges. If you want, I can scope the minimal IAM policy to exact ARNs for your account, or produce a PowerShell script that automates AWS CLI install + aws configure with prompts.\n"},{"uri":"/2-proposal/","title":"Proposal 2HTD-LearningHub","tags":[],"description":"","content":"1. Project Summary 2HTD-LearningHub is a web platform for studying, practice, exam creation and online testing. The goal is to provide a one-stop experience for students and instructors: question bank, practice exercises, mock exams and reporting.\nThe current release removes the live-class component; instead the product focuses on authoring (exam creation), submissions, automated grading and content distribution.\nThe architecture is hybrid: frontend hosted on Vercel; backend business logic runs on AWS Lambda (invoked via API Gateway); relational data is stored in SQL Server running on Amazon EC2 in a private subnet; media and documents are stored in Amazon S3.\nThe design emphasizes security and operability: network isolation with VPC/subnets, access control with IAM and Cognito, observability via CloudWatch, and EC2 administration through SSM rather than RDP.\n2. Problem Statement Learners and instructors currently rely on multiple disconnected tools (quiz builders, video calls, storage), causing fragmented workflows and management overhead. Instructors need more efficient, controlled ways to author, review and distribute exam content. Systems can degrade during peak events (mock exams) and struggle to keep latency low for all users. Real-time notifications and reporting are limited for course management and student tracking. Exam content and user data must be protected against leaks and abuse. 3. Solution \u0026amp; Architecture The solution maps to the architecture diagram and includes:\nFrontend: Vercel (hosting SPA/static), DNS managed in Route 53. Authentication: Amazon Cognito for user pools (signup/login) and token-based authentication. API layer: Amazon API Gateway (HTTP API) as the public endpoint, routing to AWS Lambda (proxy). Backend: AWS Lambda (Node.js) for business logic. Database: Amazon EC2 (Windows + SQL Server Express) in a private subnet as the primary datastore for users, courses, quizzes and results. Storage: Amazon S3 for media and large files; the database stores metadata and S3 links. Network: VPC with public/private subnets, NAT Gateway and Internet Gateway; EC2 sits in private subnet; Lambda may run in VPC when DB access is required. Operations \u0026amp; Security: IAM roles, CloudWatch logs, and AWS Systems Manager (SSM) for accessing EC2 without RDP. Architecture Diagram Figure 1: 2HTD-LearningHub — Frontend (Vercel), API Gateway → Lambda, EC2 (SQL Server) in private subnet, S3 for storage\nAWS Services Used Amazon EC2 (Windows + SQL Server Express): Run SQL Server (SQLEXPRESS) as the primary database for the system (users, courses, quizzes, tests, results).\nAmazon S3: Store lecture files, videos, PDFs and images; the database stores metadata and S3 links.\nAWS Lambda: Run backend Node.js processes (API handlers, background tasks).\nAmazon API Gateway (HTTP API): Public HTTPS endpoint for the frontend (Vercel) to call into the backend.\nAmazon Cognito: User sign-up / sign-in, email verification, provides AccessToken/IdToken.\nAWS IAM: Permissions for Lambda to access S3, EC2, SSM.\nAmazon CloudWatch Logs: Store logs from Lambda and API Gateway for debugging and monitoring.\nAWS Systems Manager (SSM): Session Manager to access EC2 without RDP and run PowerShell/sqlcmd.\nAmazon Route 53: DNS for pointing to Vercel deployment.\nVPC, Subnets, NAT Gateway, Internet Gateway: Network isolation and secure access to resources.\nComponent Design VPC: learninghub-vpc with public \u0026amp; private subnets. EC2 DB in private subnet; NAT Gateway for outbound from private. EC2 (DB): Windows Server + SQL Server Express on EC2 instance (private), storing primary data. Backups via snapshots and maintenance policies. Lambda (Backend): Handle API requests from API Gateway; may be placed in VPC for DB access. API Gateway: HTTP API routing to Lambda (proxy). Enable CORS for the frontend. S3: Store files and provide presigned URLs for uploads/downloads. Cognito: User Pool for authentication and email/OTP verification. IAM \u0026amp; Roles: following least-privilege principle. SSM: Use Session Manager to connect to EC2, run sqlcmd or PowerShell for DB administration. CloudWatch: Collect logs and metrics; dashboards and alerts for operations. 4. Technical Implementation IaC \u0026amp; Provisioning: Use Terraform or AWS CDK to provision VPC, subnets, NAT/IGW, EC2 (Windows + SQL Server), S3 bucket, API Gateway, Lambda, Cognito, IAM roles, SSM and CloudWatch. CI/CD: GitHub Actions to build \u0026amp; deploy frontend (Vercel), deploy Lambda packages/containers, and apply IaC changes to staging/production. API \u0026amp; Data: API Gateway (HTTP API) → Lambda (Node.js) handles CRUD for courses/quizzes/tests; Lambda connects to EC2 SQL Server on the private network. EC2 database is managed via SSM for migrations and backups. File Storage: Teacher upload flow — backend issues presigned URL → frontend uploads directly to S3; DB stores metadata and link. Security \u0026amp; Operations: Least-privilege IAM, KMS encryption as needed, Secrets Manager/SSM Parameter Store for credentials; CloudWatch logs \u0026amp; alarms, backup snapshots for EC2, and runbooks for incident response. AWS Systems Manager (SSM): Use Session Manager to access and administer EC2 instances securely (run PowerShell/sqlcmd for migrations, connectivity checks, backups/restores and troubleshooting) without opening extra network ports. 5. Timeline \u0026amp; Milestones Week 1-2: Detailed design, data model and IaC scaffold; provision EC2 DB. Week 3-4: Implement Cognito, API Gateway + Lambda, test DB connectivity via SSM. Week 5-6: Implement authoring, submission and grading; integrate S3 upload flow. Week 7: System testing, backups and security review. Week 8: Load testing, observability dashboards, documentation and handover. 6. Budget Estimation AWS Service Key Billing Factors Estimated Cost (USD) EC2 + EBS (Database) Windows + SQL Server Express (instance + EBS storage, snapshots) 23.00 Amazon S3 Storage (GB) and requests (PUT/GET) 1.30 AWS Lambda Invocations and compute (low starter estimate) 1.00 Amazon API Gateway (HTTP API) Request charges (per-request pricing) 2.50 Amazon CloudWatch Logs Log ingestion \u0026amp; storage for Lambda/API 1.50 NAT Gateway Hourly + data processing (egress) — often the largest cost driver 30.00 Route 53 Hosted zone + DNS queries 1.00 Amazon Cognito + IAM + SSM Authentication, IAM operations, SSM Session Manager (no direct charge) 0.00 TOTAL ≈ 60.00 Note: Actual costs depend on traffic, region and configuration; costs can be reduced further by turning off dev/test environments or optimizing egress.\n7. Risk Assessment Risk Impact Mitigation EC2 / egress costs High Monitor usage, choose right instance size, schedule/auto-stop dev instances. Data leaks / credentials exposure Very High Use Secrets Manager, KMS, least-privilege IAM, periodic pentests. Lambda ↔ EC2 connectivity issues Medium Proper security groups, test in staging, consider a proxy if needed. DB backup/recovery failures High Automated snapshots, tested restore procedures. 8. Expected Outcomes An integrated platform for studying, practice, exam creation and online testing that reduces tool fragmentation for learners and instructors. Automation of authoring, distribution and grading to improve instructor productivity. A stable, secure, and scalable infrastructure; EC2-hosted SQL Server ensures compatibility with existing database requirements. S3-based storage for large files to reduce DB load and simplify backups; Lambda reduces operational overhead for business logic. Observability (CloudWatch) and logging to support troubleshooting and performance monitoring. "},{"uri":"/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Get familiar with and practice AI/ML services in the AWS ecosystem. Understand the training, deployment and consumption lifecycle for Machine Learning models. Hands-on with SageMaker, Rekognition, Comprehend, and Kendra. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Overview of AI/ML on AWS - Learn ML-supporting services: SageMaker, Rekognition, Comprehend, Kendra, Translate, Polly 10/11/2025 10/11/2025 AWS Journey 3 - Hands-on with Amazon SageMaker: + Create a Notebook Instance + Train a simple model (Linear Regression / Image Classification) + Deploy an endpoint and test predictions 11/11/2025 11/11/2025 AWS Journey 4 - Explore Amazon Rekognition - Demo face \u0026amp; object recognition in images/videos - Integrate Rekognition API into a small web app 12/11/2025 12/11/2025 AWS Journey 5 - Practice Amazon Comprehend (NLP) - Try Amazon Kendra (contextual search) - Compare strengths and limitations of each service 13/11/2025 13/11/2025 AWS Journey 6 - Week wrap-up: + ML development lifecycle on AWS + Real-world AI/ML applications + Write a summary report and next-step recommendations 14/11/2025 14/11/2025 AWS Journey Week 10 Achievements: Understood the AWS AI/ML ecosystem and its core services.\nSuccessfully trained and deployed a basic ML model on SageMaker.\nApplied Rekognition, Comprehend, and Kendra to practical tasks.\nLearned the end-to-end ML training → deploy → integrate workflow on AWS.\n"},{"uri":"/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Learn and practice Modernization \u0026amp; Serverless Architecture on AWS. Build, deploy and manage Serverless applications using Lambda, API Gateway, DynamoDB, Cognito and SAM. Understand how to split a monolith into microservices to improve scalability and maintainability. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Introduce Modernization and Serverless concepts - Compare monolithic vs microservices architectures - Analyze benefits of moving to serverless 17/11/2025 17/11/2025 AWS Journey 3 - Hands-on with AWS Lambda: create functions, configure triggers, view logs in CloudWatch - Implement basic API logic with Lambda 18/11/2025 18/11/2025 AWS Journey 4 - Integrate API Gateway with Lambda to build REST APIs - Connect data with DynamoDB (CRUD operations) - Test APIs using Postman 19/11/2025 19/11/2025 AWS Journey 5 - Configure Cognito for user authentication (user pool, tokens) - Integrate Cognito authentication with API Gateway - Manage access via IAM Roles 20/11/2025 20/11/2025 AWS Journey 6 - Deploy the full Serverless application using AWS SAM (Serverless Application Model) - Test, collect logs and optimize performance - Week summary and report 21/11/2025 21/11/2025 AWS Journey Week 11 Achievements: Understood Serverless Architecture and benefits in cost, performance and scalability.\nDeployed a complete Serverless app using Lambda, API Gateway, DynamoDB and Cognito.\nLearned to use AWS SAM for managing and deploying serverless infrastructure automatically.\nPrepared the platform for Week 12 (Final summary \u0026amp; capstone project).\n"},{"uri":"/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Consolidate all knowledge gained during the previous 11 weeks. Build a final real-world project on AWS that integrates multiple services. Review and evaluate understanding and ability to apply AWS in practical problems. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review core services: EC2, S3, RDS, DynamoDB, IAM, VPC, Lambda, CloudWatch, CloudFront, API Gateway - Define requirements and architecture for final project 24/11/2025 24/11/2025 AWS Journey 3 - Start project implementation: + Design VPC, subnets, security groups + Configure S3, CloudFront, RDS/DynamoDB (depending on project) 25/11/2025 25/11/2025 AWS Journey 4 - Continue implementation: + Build backend with Lambda/API Gateway or EC2 (depending on architecture) + Connect databases and process data + Integrate CloudWatch logging 26/11/2025 26/11/2025 AWS Journey 5 - Finalize project: + Add Cognito authentication if needed + Complete CI/CD pipeline (CodePipeline/CodeBuild) + Perform end-to-end system tests 27/11/2025 27/11/2025 AWS Journey 6 - Write final report - Prepare presentation (architecture, reasons for service choices, cost, security) - Summarize learning journey and self-assess readiness for projects 28/11/2025 28/11/2025 AWS Journey Week 12 Achievements: Completed the final AWS project integrating multiple core services.\nDesigned, deployed, optimized and operated a practical system end-to-end.\nMastered the cloud application development lifecycle from design to production.\nFinished the training roadmap and prepared to apply skills in larger real-world projects.\n"},{"uri":"/5-workshop/5.3-cognito-api-gateway/","title":"Cognito + API Gateway","tags":[],"description":"","content":"Overview\nThis lab shows how to secure HTTP APIs using Amazon Cognito and API Gateway (HTTP API). Students will create a Cognito User Pool, an App Client, and configure a JWT authorizer in API Gateway so Lambda-backed endpoints require a valid Cognito token.\nPrerequisites\nAWS account with permission to create Cognito, API Gateway, Lambda, IAM roles. AWS CLI or Console access. A simple Lambda or HTTP backend (the workshop provides a sample handler). Learning objectives\nCreate and configure an Amazon Cognito User Pool and App Client. Configure an API Gateway HTTP API with JWT authorizer pointing to the Cognito User Pool. Protect endpoints and test access using Cognito tokens. Integrate login flow from frontend to call protected APIs. Lab steps (high level)\nCreate a Cognito User Pool Use the Console or AWS CLI to create a User Pool with email as username. Enable sign-up and create a test user. Create an App Client Create an App Client (no client secret for single-page apps) and configure callback/logout URLs for the frontend. Configure a domain (optional) Use a Cognito hosted domain or bring your own via Route 53. Deploy or identify your API backend Ensure Lambda function(s) exist and are callable by API Gateway. Create an API Gateway (HTTP API) Create HTTP API and add an integration to the Lambda backend. Add a route (e.g. POST /upload or GET /items). Add JWT Authorizer In API Gateway, create a JWT authorizer referencing the Cognito User Pool issuer and audience (App Client ID). Protect the route using the JWT authorizer. Test authentication flow Use the Cognito hosted UI to sign in and obtain an ID/Access token, or use the Admin Initiate Auth API for a test user. Call the protected endpoint with Authorization: Bearer \u0026lt;access_token\u0026gt;. Integrate from frontend Use @aws-amplify/auth or fetch to call Cognito login and attach the token to requests. Testing \u0026amp; troubleshooting\nVerify token audience (aud) matches the App Client ID. Check CORS settings on API Gateway for browser calls. Inspect CloudWatch Logs for Lambda and API Gateway execution errors. Cleanup\nDelete the Cognito User Pool, App Client, and API Gateway resources when finished. References\nAWS Cognito Console API Gateway (HTTP API) JWT authorizers "},{"uri":"/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - Accelerate AI development with Amazon Bedrock API keys This blog introduces how to accelerate your AI application development by integrating and managing Amazon Bedrock API keys within your microservices architecture. You will learn why Bedrock API keys are a crucial tool for accessing and utilizing diverse AI models (such as Claude, Titan, Llama 2, and others) from multiple providers, and how microservices help make your system flexible, scalable, and easier to manage API access. The article will also guide you through the steps to set up the environment, organize the Bedrock API key access pipeline, and ensure compliance with security and IAM (Identity and Access Management) standards when working with AI.\nBlog 2 - New features and developer experience with enhanced Amazon Location Service This blog introduces how to leverage new features and the enhanced developer experience with Amazon Location Service. You will learn why these new updates are crucial for storing and analyzing diverse location data (such as maps, POIs, IoT device tracking data, etc.), how the new features help make your system flexible, scalable, and easier to integrate location services. The article will also guide you through the steps to set up the environment, organize the location data processing pipeline, and ensure compliance with security and privacy standards when using this service.\nBlog 3 - Accelerate VMware to AWS Migration with Trianz’s Rapid Migration Offer and Concierto platform This blog introduces how to accelerate the migration of VMware workloads to AWS by leveraging Trianz’s Rapid Migration Offer and the Concierto platform. You will learn why moving VMware to AWS helps organizations achieve higher scalability, cost optimization, and operational resilience. The article also explores how Trianz’s Concierto platform automates and simplifies the migration process—from discovery and assessment to deployment and optimization—ensuring minimal downtime and seamless transition. In addition, you will be guided through best practices for post-migration management, monitoring, and cost governance to maximize the benefits of AWS infrastructure.\n"},{"uri":"/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"Event name: Vietnam Cloud Day 2025 — Ho Chi Minh City Connect (Builders edition)\nTime: Thursday, 18 September 2025, 09:00 – 17:00\nLocation: Ho Chi Minh City\nRole: Attendee\nDescription: The event provided a broad perspective on cloud transformation in Vietnam, covering migration, modernization, security, and GenAI-driven development.\nKey highlights:\nOpening keynote on AWS strategic direction for Vietnam Practical experience sharing from Techcombank and U2U Network Panel session “GenAI Revolution” with industry leaders Migration \u0026amp; Modernization track with real-world case studies of workload moves to AWS Demo of Amazon Q Developer for automating parts of the SDLC Technical session on modernizing VMware workloads to AWS The session delivered useful insights for applying cloud modernization and GenAI in real projects, and provided good networking opportunities.\nEvent 2 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"/5-workshop/5.4-lambda-api-gateway/","title":"Lambda + API Gateway","tags":[],"description":"","content":"Overview\nThis lab demonstrates how to build and deploy Lambda-backed HTTP APIs using Amazon API Gateway (HTTP API). Students will create a production-representative Lambda service (Node.js), integrate it with HTTP API routes, configure networking so Lambda can access an EC2-hosted SQL Server in a private subnet, secure credentials with Secrets Manager, and add observability (CloudWatch + X-Ray). The lab also covers IAM roles, CORS, payload format versions, cold-start considerations and deployment examples using AWS CLI / PowerShell.\nPrerequisites\nAWS account with permission to create Lambda, API Gateway, IAM roles, CloudWatch, VPC resources and Secrets Manager. AWS CLI v2 (or PowerShell configured) and optional SAM/Serverless framework for local testing. Node.js (LTS) for function development and bundling. Architecture notes\nFrontend (Vercel) calls API Gateway (HTTP API) endpoints. API Gateway routes requests to Lambda functions (Node.js). When Lambdas need to access SQL Server on EC2 (private subnet), configure Lambda to run in the same VPC subnets and allow network access via Security Groups and SSM if needed. Credentials (DB user/password) are stored in Secrets Manager; Lambda retrieves them at runtime (with IAM permission secretsmanager:GetSecretValue). Detailed steps\nPrepare Lambda execution role (IAM) Create a role with a trust policy for Lambda and attach the following minimal managed policies or inline policies: Example trust policy (lambda assume role):\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: {\u0026#34;Service\u0026#34;: \u0026#34;lambda.amazonaws.com\u0026#34;}, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } Attach policies (examples):\nAWSLambdaBasicExecutionRole (CloudWatch Logs) Inline policy for Secrets Manager + ENI/VPC actions (example): { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ {\u0026#34;Effect\u0026#34;:\u0026#34;Allow\u0026#34;,\u0026#34;Action\u0026#34;:[\u0026#34;secretsmanager:GetSecretValue\u0026#34;,\u0026#34;secretsmanager:DescribeSecret\u0026#34;],\u0026#34;Resource\u0026#34;:\u0026#34;arn:aws:secretsmanager:\u0026lt;region\u0026gt;:\u0026lt;account-id\u0026gt;:secret:\u0026lt;your-secret\u0026gt;\u0026#34;}, {\u0026#34;Effect\u0026#34;:\u0026#34;Allow\u0026#34;,\u0026#34;Action\u0026#34;:[\u0026#34;ec2:CreateNetworkInterface\u0026#34;,\u0026#34;ec2:DescribeNetworkInterfaces\u0026#34;,\u0026#34;ec2:DeleteNetworkInterface\u0026#34;],\u0026#34;Resource\u0026#34;:\u0026#34;*\u0026#34;} ] } PowerShell / AWS CLI example to create role and attach policy (PowerShell):\n# create role $trust = \u0026#39;{\u0026#34;Version\u0026#34;:\u0026#34;2012-10-17\u0026#34;,\u0026#34;Statement\u0026#34;:[{\u0026#34;Effect\u0026#34;:\u0026#34;Allow\u0026#34;,\u0026#34;Principal\u0026#34;:{\u0026#34;Service\u0026#34;:\u0026#34;lambda.amazonaws.com\u0026#34;},\u0026#34;Action\u0026#34;:\u0026#34;sts:AssumeRole\u0026#34;}]}\u0026#39; aws iam create-role --role-name LearningHubLambdaRole --assume-role-policy-document $trust # attach managed policy aws iam attach-role-policy --role-name LearningHubLambdaRole --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole # put inline policy for secrets + ENI # save the inline JSON to lambda-secrets-policy.json locally then run: aws iam put-role-policy --role-name LearningHubLambdaRole --policy-name LambdaSecretsPolicy --policy-document file://lambda-secrets-policy.json Build and package Lambda (Node.js) Project layout (example):\nlambda/ package.json index.js node_modules/\nUse a bundler (esbuild/webpack) or npm install --production and zip the folder.\nPowerShell example to package:\ncd lambda npm install --production Compress-Archive -Path * -DestinationPath ..\\lambda-package.zip Create Lambda function (CLI) aws lambda create-function --function-name learninghub-api-handler \\ --runtime nodejs18.x --handler index.handler \\ --zip-file fileb://lambda-package.zip \\ --role arn:aws:iam::\u0026lt;account-id\u0026gt;:role/LearningHubLambdaRole \\ --timeout 30 --memory-size 512 If Lambda needs DB access in private subnet, update function configuration to include VPC config (subnet IDs + security group IDs):\naws lambda update-function-configuration --function-name learninghub-api-handler --vpc-config SubnetIds=subnet-aaa,subnet-bbb,SecurityGroupIds=sg-xxxx Notes on VPC:\nWhen Lambda runs in a VPC it creates ENIs in the subnets; this can increase cold-start time. Keep at least two ENI-capable subnets (private) available. If Lambda needs internet access (e.g., to call external APIs), ensure private subnets have a route to a NAT Gateway in a public subnet. Store DB credentials in Secrets Manager and grant Lambda access Create secret (PowerShell):\naws secretsmanager create-secret --name learninghub/sqlserver --secret-string \u0026#39;{\u0026#34;username\u0026#34;:\u0026#34;dbuser\u0026#34;,\u0026#34;password\u0026#34;:\u0026#34;P@ssw0rd\u0026#34;,\u0026#34;host\u0026#34;:\u0026#34;10.0.1.10\u0026#34;,\u0026#34;port\u0026#34;:\u0026#34;1433\u0026#34;}\u0026#39; Attach IAM permission (see step 1) so Lambda can GetSecretValue for that secret.\nCreate API Gateway (HTTP API) and integrate with Lambda Create HTTP API (CLI):\n$api=$(aws apigatewayv2 create-api --name learninghub-api --protocol-type HTTP --output json) $apiId=(ConvertFrom-Json $api).ApiId # create integration to Lambda $lambdaArn = \u0026#34;arn:aws:lambda:\u0026lt;region\u0026gt;:\u0026lt;account-id\u0026gt;:function:learninghub-api-handler\u0026#34; $integration=$(aws apigatewayv2 create-integration --api-id $apiId --integration-type AWS_PROXY --integration-uri $lambdaArn --payload-format-version 2.0 --output json) $integrationId=(ConvertFrom-Json $integration).IntegrationId # create a route aws apigatewayv2 create-route --api-id $apiId --route-key \u0026#34;GET /status\u0026#34; --target \u0026#34;integrations/$integrationId\u0026#34; # add permission so API Gateway can invoke Lambda aws lambda add-permission --function-name learninghub-api-handler --statement-id apigw-invoke --action lambda:InvokeFunction --principal apigateway.amazonaws.com --source-arn arn:aws:execute-api:\u0026lt;region\u0026gt;:\u0026lt;account-id\u0026gt;:$apiId/*/* # deploy aws apigatewayv2 create-stage --api-id $apiId --stage-name prod --auto-deploy Notes:\nUse payload-format-version 2.0 for HTTP API; event shape differs from REST API. integration-type AWS_PROXY maps the entire HTTP request to Lambda event. Configure CORS (HTTP API) You can enable CORS in console or with CLI using update-api to set CorsConfiguration:\n$cors=\u0026#39;{\u0026#34;AllowOrigins\u0026#34;:[\u0026#34;https://learninghub.example.com\u0026#34;],\u0026#34;AllowMethods\u0026#34;:[\u0026#34;GET\u0026#34;,\u0026#34;POST\u0026#34;,\u0026#34;OPTIONS\u0026#34;],\u0026#34;AllowHeaders\u0026#34;:[\u0026#34;Content-Type\u0026#34;,\u0026#34;Authorization\u0026#34;],\u0026#34;MaxAge\u0026#34;:3600}\u0026#39; aws apigatewayv2 update-api --api-id $apiId --cors-configuration $cors Test the endpoint PowerShell example:\nInvoke-RestMethod -Method Get -Uri \u0026#34;https://$apiId.execute-api.\u0026lt;region\u0026gt;.amazonaws.com/status\u0026#34; If your route is protected (e.g., by Cognito authorizer) attach token header: -Headers @{ Authorization = \u0026quot;Bearer $token\u0026quot; }.\nObservability \u0026amp; best practices CloudWatch Logs: set log retention, enable structured logging (JSON) from Lambda. X-Ray: enable active tracing on the Lambda function for distributed tracing. Metrics \u0026amp; Alarms: create alarms for Errors, Throttles, Duration. API access logs: configure stage-level access logging for HTTP API via accessLogSettings in create-stage/update-stage. Security Use Secrets Manager for DB credentials; avoid environment variables with plaintext secrets. Scope IAM policies to least privilege and specific ARNs when possible. When running Lambda in VPC, ensure security group rules allow outbound to the DB (port 1433) and inbound rules on the DB EC2 allow only Lambda SGs. Cleanup # delete stage and API aws apigatewayv2 delete-api --api-id $apiId # delete lambda aws lambda delete-function --function-name learninghub-api-handler # delete role inline policy, detach managed, then delete role aws iam delete-role-policy --role-name LearningHubLambdaRole --policy-name LambdaSecretsPolicy aws iam detach-role-policy --role-name LearningHubLambdaRole --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole aws iam delete-role --role-name LearningHubLambdaRole Appendix: sample minimal Lambda (Node.js)\n// index.js const sql = require(\u0026#39;mssql\u0026#39;); // if using mssql package exports.handler = async (event) =\u0026gt; { // parse request, read secret, connect to SQL Server, run query return { statusCode: 200, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39; }, body: JSON.stringify({ message: \u0026#39;OK\u0026#39;, request: event }) }; }; References\nAWS Lambda Developer Guide Amazon API Gateway (HTTP API) Developer Guide AWS Secrets Manager Developer Guide "},{"uri":"/5-workshop/5.5-ec2-privatesubnet/","title":"EC2 (Private Subnet) — Windows + SQL Server","tags":[],"description":"","content":"Overview\nThis lab covers deploying and operating a Windows EC2 instance running SQL Server Express in a private subnet designed for internal-only database access. The document focuses on network design (VPC, route tables, NAT, VPC endpoints), secure administration via AWS Systems Manager (SSM), SQL Server silent installation and hardening, backup/restore (EBS snapshots and SQL backups to S3), monitoring, and production-ready operational practices.\nPrerequisites\nAWS account with permissions to create VPCs, subnets, route tables, NAT Gateways, EC2, IAM roles, Systems Manager, EBS, and S3. AWS CLI v2 or Console access and basic familiarity with EC2, VPC and IAM concepts. An existing VPC or permission to create one for the lab. Learning objectives\nDesign a VPC with public and private subnets and correct routing for private DB instances. Launch a Windows EC2 instance in private subnets and configure it securely for SQL Server. Use AWS Systems Manager (Session Manager / Run Command / Patch Manager) for administration without exposing RDP. Implement backups (EBS snapshots and SQL backups), monitoring, and basic hardening. Network \u0026amp; VPC design\nRecommended subnet layout:\nPublic subnets: NAT Gateway, load balancers, bastion (if used). Private subnets: database instances, application services (Lambda or ECS tasks in private subnets). At least two AZs with private subnets for resilience. Route tables:\nPublic route table: route 0.0.0.0/0 -\u0026gt; Internet Gateway (IGW). Private route table: route 0.0.0.0/0 -\u0026gt; NAT Gateway (for OS updates/outbound), or no NAT if using SSM/VPC endpoints for patching. VPC Endpoints:\nAdd Interface endpoints for com.amazonaws.\u0026lt;region\u0026gt;.ssm, com.amazonaws.\u0026lt;region\u0026gt;.ec2messages, com.amazonaws.\u0026lt;region\u0026gt;.ssmmessages to allow SSM traffic without Internet. Consider Gateway endpoint for S3 if you push backups to S3 (reduces public egress). Security groups \u0026amp; NACLs\nSecurity Group for SQL Server (db-sg):\nInbound: TCP 1433 from application subnets/SGs only (use Security Group IDs for source when possible). Outbound: restrict as needed; at minimum allow response traffic. Management Security Group (mgmt-sg):\nAllow SSM related traffic (SSM uses agent outbound to AWS endpoints; with VPC endpoints you don\u0026rsquo;t need IGW/NAT for SSM). Network ACLs: keep default permissive rules or implement stateless filters if required by policy; Security Groups are primary control.\nIAM \u0026amp; instance profile\nCreate an IAM role for EC2 with the following managed policy to enable SSM:\nAmazonSSMManagedInstanceCore Additional policies (add least privilege scope):\nPermission to create \u0026amp; describe EBS snapshots if instance will trigger snapshots: ec2:CreateSnapshot, ec2:DescribeVolumes, etc. Permission to upload backups to S3 if using instance-based uploads (consider using a separate S3 role/policy limited to a prefix). Instance type, storage, and AMI choices\nInstance type: choose based on SQL Server workloads (t3.medium or t3.large for light dev/test; r5/ m5 for heavier loads). For production, consider dedicated RDS or EC2 with provisioned IOPS. Storage: Separate volumes: OS (C:) and Data (E: or D:) volumes. Put SQL Data, Logs, TempDB on separate EBS volumes for performance and snapshot granularity. Use gp3 or io2 for consistent IOPS depending on workload. Enable EBS encryption (KMS) for sensitive data. Windows \u0026amp; SQL Server installation (automation)\nInstall via SSM Run Command or using a pre-baked AMI with SQL Server/agents installed. Example PowerShell sequence (SSM Run Command or Session Manager): PowerShell (on instance) - silent install example for SQL Server Express:\n# Download installer $url = \u0026#34;https://download.microsoft.com/.../SQLEXPR_x64_ENU.exe\u0026#34; Invoke-WebRequest -Uri $url -OutFile C:\\Temp\\sqlexpr.exe # Create configuration file (simple example) $config = @\u0026#34; [OPTIONS] ACTION=Install FEATURES=SQLENGINE INSTANCENAME=SQLEXPRESS SECURITYMODE=SQL SAPWD=\u0026#34;P@ssw0rd\u0026#34; SQLSVCACCOUNT=\u0026#34;NT AUTHORITY\\SYSTEM\u0026#34; TCPENABLED=1 NPENABLED=0 \u0026#34;@ $config | Out-File C:\\Temp\\ConfigurationFile.ini -Encoding ascii # Run silent installer \u0026amp; C:\\Temp\\sqlexpr.exe /Q /ACTION=Install /IACCEPTSQLSERVERLICENSETERMS /ConfigurationFile=C:\\Temp\\ConfigurationFile.ini After install, enable TCP/IP protocol in SQL Server Configuration Manager (can be scripted via PowerShell/registry) and restart SQL Server service. Configure SQL Server Mixed Mode if you need SQL authentication; create a dedicated SQL login for app access. Firewall \u0026amp; Windows Defender\nConfigure Windows Firewall to allow inbound TCP 1433 only from trusted source ranges or SGs (if using security group references via AWS-managed firewall integration, still set host firewall). Administration via SSM\nUse Session Manager for interactive shell access and port forwarding to avoid exposing RDP. Example to start a session: aws ssm start-session --target i-0123456789abcdef0 Port forwarding example (local RDP tunneled over Session Manager): Start-SSMSession -Target i-0123456789abcdef0 -DocumentName AWS-StartPortForwardingSession -Parameters @{\u0026#34;portNumber\u0026#34;=[\u0026#34;3389\u0026#34;];\u0026#34;localPortNumber\u0026#34;=[\u0026#34;13389\u0026#34;]} Connectivity from Lambda or app\nIf using Lambda in the same VPC, ensure Lambda\u0026rsquo;s vpcConfig includes private subnet IDs and security group IDs that allow outbound to db-sg on 1433. Remember Lambda will create ENIs in the subnets which affects cold-start.\nTesting connectivity:\nFrom a bastion or EC2 in same subnet: Test-NetConnection -ComputerName 10.0.x.10 -Port 1433 (PowerShell) Or use tcping/telnet for TCP checks. Backups and restore\nTwo layers of backups recommended:\nSQL-native backups: full/diff/log backups scheduled and stored to disk, then copied to S3 (use SSM or custom backup agent/script). Infrastructure backups: EBS snapshots for volume-level recovery (fast restore). Automate snapshots with AWS Data Lifecycle Manager (DLM) or Lambda triggered by CloudWatch Events.\nRestore test: create volume from snapshot, attach to a recovery instance, mount and verify SQL files or restore .bak files to a new SQL Server instance.\nMonitoring, logging, and patching\nInstall and configure CloudWatch agent to collect Windows performance counters and custom logs. Enable CloudWatch Logs for SSM and EC2; set log retention and metric filters for alerts. Use SSM Patch Manager to apply OS updates on a maintenance window without opening internet-facing management ports. Hardening \u0026amp; security\nPrinciple of least privilege for IAM roles and S3 prefixes. Use Security Group references (SG ID) rather than CIDR when restricting DB access. Enable EBS encryption and consider using a customer-managed KMS key when required. Disable unnecessary Windows features and remove local admin accounts where possible; use AWS IAM Identity Center or AD integration for enterprise setups. Troubleshooting checklist\nSSM connectivity: check SSM agent, instance role, VPC endpoints for SSM, and CloudWatch logs for agent errors. Network: verify route tables, NACLs, SG rules, and source/destination checks for NAT instances. SQL: check SQL Server error logs (in C:\\Program Files\\Microsoft SQL Server\\MSSQL..\\MSSQL\\Log), verify TCP/IP enabled and SQL service running. Cleanup\nTerminate EC2 instance, delete EBS snapshots and any temporary S3 objects, remove IAM role/policies and DLM lifecycle policies created for snapshots. References \u0026amp; further reading\nAWS Systems Manager Session Manager Amazon VPC design and NAT Gateway documentation SQL Server Express silent install and configuration guidance AWS Data Lifecycle Manager (DLM) for EBS snapshots "},{"uri":"/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Workshop — LearningHub (2HTD) Overview This workshop accompanies the 2HTD-LearningHub proposal and provides step-by-step labs to deploy and validate the project\u0026rsquo;s main components: frontend hosted on Vercel, serverless backend with AWS Lambda + API Gateway, relational data stored in SQL Server on EC2 (private subnet), and media stored in Amazon S3. The labs focus on secure hybrid access patterns, VPC design, EC2 administration via AWS Systems Manager (SSM), presigned S3 uploads, Lambda-based APIs, and Cognito authentication.\nLabs are short and focused; they can run in a compact test environment (single VPC) or in an expanded setup (cloud VPC + on-prem simulation) to demonstrate different network flows and NAT/egress trade-offs.\nLearning objectives Understand the LearningHub hybrid architecture (Vercel + Lambda + EC2 + S3). Provision VPCs and subnets, and design NAT / VPC endpoints to control egress costs. Deploy EC2 Windows + SQL Server Express and manage it securely using AWS Systems Manager (SSM). Configure S3 buckets and implement uploads using presigned URLs from the frontend. Deploy Lambda (Node.js) functions and expose them via API Gateway (HTTP API). Configure Amazon Cognito for authentication and test protected API flows. Project Architecture The LearningHub system uses a frontend-hosted + serverless backend pattern with a Microsoft SQL Server running on an EC2 instance inside a private subnet for relational data. Key components:\nFrontend: Static or SSR Next.js app hosted on Vercel, serving the UI and calling APIs. API: Amazon API Gateway (HTTP API) forwarding requests to AWS Lambda (Node.js) for business logic. Storage: Amazon S3 for media storage; uploads use presigned URLs from the frontend. Database: Microsoft SQL Server on Amazon EC2 in a private subnet (accessed from Lambda via private networking or via approved routes). Authentication: Amazon Cognito issues tokens for frontend authentication and protects API endpoints. Network \u0026amp; Security: VPC, subnets, security groups, IAM roles, and VPC Endpoints for S3 to reduce egress; NAT Gateway for egress where required. Ops \u0026amp; Observability: AWS Systems Manager (SSM) for EC2 administration, CloudWatch for logs/metrics, and AWS Secrets Manager / Parameter Store for secrets. Main Request / Data Flows The user loads the frontend from Vercel; if authentication is required the frontend redirects to Cognito and receives an access token. For media upload, the frontend requests a presigned URL from an API endpoint (API Gateway → Lambda), passing the user\u0026rsquo;s token where required. The Lambda function generates a presigned S3 URL using the SDK and returns it; the frontend uploads directly to S3 with the presigned URL. After upload, the frontend may call a confirmation API (API Gateway → Lambda) to record metadata or trigger downstream processing. Business requests (CRUD operations, user data) go through API Gateway → Lambda; Lambdas may query or update the SQL Server on EC2 over the private network or use S3 as an intermediary for large objects. EC2 management tasks (SQL setup, patching) are performed via AWS Systems Manager (SSM Session Manager) without opening RDP/SSH to the internet. To minimize egress costs when accessing S3 from inside the VPC, enable an S3 VPC Endpoint; for external package downloads or other internet egress, consider a NAT Gateway (cost trade-off) or use SSM + endpoints. These flows map directly to the workshop labs: presigned S3 upload, S3 access from VPC (with endpoints), EC2 management via SSM, and Lambda APIs connecting to EC2/DB.\nAWS Services Used Category Service Compute AWS Lambda (Function / Container), Amazon EC2 (Windows + SQL Server Express) Storage Amazon S3 API Amazon API Gateway (HTTP API) Authentication \u0026amp; Security Amazon Cognito, AWS IAM, AWS Secrets Manager Monitoring Amazon CloudWatch Logs \u0026amp; Metrics Network VPC, Subnets, NAT Gateway, Internet Gateway, VPC Endpoints DNS Amazon Route 53 IaC / CI-CD Terraform (or CDK), GitHub Actions Ops \u0026amp; Backup AWS Systems Manager (SSM), EBS Snapshots Time \u0026amp; Cost Estimate Item Detail Time 3–4 hours (short lab) Level Intermediate Estimated Cost Workshop: minimal (temporary lab resources); Full deployment (production): ≈ 60.00 USD / month (see Proposal) Note: Workshop costs depend on runtime hours and resource choices. The full-system monthly estimate is available in the Proposal section.\nLab index Workshop overview Prerequisite \u0026amp; environment setup Integration Cognito + API Gateway Lambda + API Gateway (Serverless API) EC2 Windows + SQL Server (Private Subnet) Cleanup "},{"uri":"/5-workshop/5.6-cleanup/","title":"Clean up","tags":[],"description":"","content":"Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "},{"uri":"/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nDuring my internship at [Company/Organization Name] from [start date] to [end date], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [briefly describe the main project or task], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "},{"uri":"/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nHere, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "},{"uri":"/1-worklog/1.2-week2/","title":"","tags":[],"description":"","content":"title: \u0026ldquo;Week 2 Worklog\u0026rdquo; weight: 1 chapter: false pre: \u0026quot; 1.2. \u0026quot;\nWeek 2 — Objectives Get hands-on with S3 static website hosting, RDS (MySQL) and Route53 basics. Learn how to connect an EC2 client to an RDS instance and configure IAM/Security Groups for DB access. Planned tasks this week Day Task Start Date Completion Date Reference Material 2 - Create an S3 bucket to host a static website\n- Upload demo HTML/CSS files to the bucket 15/09/2025 15/09/2025 AWS Journey 3 - Enable Static Website Hosting on the S3 bucket\n- Configure bucket policy to allow public read for site assets\n- Access the site via the S3 website endpoint 16/09/2025 16/09/2025 AWS Journey 4 - Launch an RDS MySQL instance (Free Tier for lab)\n- Configure VPC Security Group to allow connections from application subnets/EC2\n- Record endpoint and credentials 17/09/2025 17/09/2025 AWS Journey 5 - Launch an EC2 instance and install MySQL client/tools\n- From EC2, connect to the RDS endpoint using CLI/mysql client and create a test database/table 18/09/2025 18/09/2025 AWS Journey 6 - Learn Route53 basics and create a Hosted Zone\n- Create A / CNAME records to point a domain to the S3 static site\n- Verify domain access to the website 19/09/2025 19/09/2025 AWS Journey Week 2 — Outcomes Built and hosted a static website in S3 and verified public access via the S3 endpoint. Configured Route53 to map a domain to the S3 site (A/CNAME) and validated domain access. Created an RDS MySQL instance and connected to it from an EC2 client; recorded endpoint and credentials for testing. Learned to configure Security Groups and minimal IAM roles relevant to RDS and S3. Recommended next topics for Week 3: CloudFront, DynamoDB, and ElastiCache for caching and scale testing. "},{"uri":"/4-eventparticipated/4.1-event1/","title":"","tags":[],"description":"","content":"title: \u0026ldquo;Event 1\u0026rdquo; weight: 1 chapter: false pre: \u0026quot; 4.1. \u0026quot; VIETNAM CLOUD DAY 2025 — HO CHI MINH CITY CONNECT (BUILDERS EDITION) Event information\nTime: Thursday, 18 September 2025, 09:00 – 17:00 Location: Ho Chi Minh City Role: Attendee Overview: The event offered a comprehensive view of cloud transformation in Vietnam, highlighting migration, modernization, security and the growing role of GenAI in builder workflows.\nEvent purpose Share AWS strategy and vision for Vietnam and opportunities for builders. Provide real-world modernization stories from enterprise practitioners. Host discussions and workshops covering migration, security and GenAI-powered development. Speakers Eric Yeo — Country General Manager, AWS Dr. Jens Lottner — CEO, Techcombank Ms. Trang Phung — CEO \u0026amp; Co-Founder, U2U Network Jaime Valles — VP, GM Asia Pacific and Japan, AWS Jeff Johnson — MD, ASEAN, AWS Vũ Văn — CEO, ELSA Corp Nguyễn Hoà Bình — Chairman, Nexttech Group Dieter Botha — CEO, TymeX Hùng Nguyễn Gia — Head of Solutions Architect, AWS Phúc Nguyễn — Solutions Architect, AWS Alex Trần — AI Director, OCB And other experts from LPBank, Ninety Eight, Techcombank, and more. Notable content Opening keynote: AWS strategic direction and opportunities for young builders in Vietnam. Enterprise sharing: Techcombank and U2U Network present real migration and modernization journeys. Panel “GenAI Revolution”: leaders discuss innovation and how AI ties to business strategy. Migration \u0026amp; Modernization track: case studies of large-scale workload migration to AWS. Demo: Amazon Q Developer for automating parts of the SDLC and supporting modernization. Technical session: roadmaps for modernizing VMware-based workloads to AWS (EKS, RDS, serverless). What I learned Always start from business needs, then choose appropriate technologies. Migration requires a clear phased roadmap rather than a big-bang approach. Amazon Q Developer shows promise to accelerate the SDLC and reduce manual effort. Security must be integrated from day one (security by design). Practical applications Run event-storming sessions to identify GenAI use cases with product teams. Pilot Amazon Q Developer in internal workflows to automate tests and documentation. Apply phased migration strategies for cloud projects. Integrate DevSecOps practices to ensure security across the delivery pipeline. Personal impressions This was my first time attending Vietnam Cloud Day and I found it highly valuable: the CEO/CTO talks clarified market context and opportunities; panel discussions were practical and emphasized close ties between AI and business strategy; demos like Amazon Q Developer illustrated tangible productivity gains.\nKey takeaways GenAI is a powerful enabler but human strategy and governance remain decisive. A phased, measurable migration approach reduces risk and increases success probability. Cloud + AI can be a differentiator for Vietnamese enterprises when applied with clear roadmaps. Event photos: add images here\n"},{"uri":"/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"/tags/","title":"Tags","tags":[],"description":"","content":""}]